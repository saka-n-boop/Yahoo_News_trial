# -*- coding: utf-8 -*-
"""
çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå›½å†…8ç¤¾å¯¾å¿œç‰ˆï¼‰ - æœ€çµ‚è¨­å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼š
1. keywords.txtã‹ã‚‰å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’èª­ã¿è¾¼ã¿ã€é †æ¬¡Yahooã‚·ãƒ¼ãƒˆã«è¨˜äº‹ãƒªã‚¹ãƒˆã‚’è¿½è¨˜ (A-Dåˆ—)ã€‚
2. æŠ•ç¨¿æ—¥æ™‚ã‹ã‚‰æ›œæ—¥ã‚’ç¢ºå®Ÿã«å‰Šé™¤ã—ã€ã‚¯ãƒªãƒ¼ãƒ³ãªå½¢å¼ã§æ ¼ç´ã€‚
3. æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã—ã€è¡Œã”ã¨ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å³æ™‚åæ˜  (E-Fåˆ—)ã€‚
Â  Â  -> ã€æ”¹ä¿®æ¸ˆã€‘è¨˜äº‹æœ¬æ–‡ã®å–å¾—ã¯**1å›ã®ã¿**ã«åˆ¶é™ã€‚
Â  Â  -> ã€æ”¹ä¿®å¾Œã€‘ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®æ›´æ–°ã¯**ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œæ—¥ã‹ã‚‰3æ—¥å‰ã¾ã§ã®è¨˜äº‹ã®ã¿**ã«åˆ¶é™ã€‚
Â  Â  -> ã€æ–°è¦ãƒ­ã‚¸ãƒƒã‚¯ã€‘æœ¬æ–‡å–å¾—æ¸ˆ ã‹ã¤ 3æ—¥ä»¥å†…ã®å ´åˆã€**ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®ã¿æ›´æ–°**ï¼ˆæœ¬æ–‡æ›´æ–°ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚
Â  Â  -> ã€æ”¹ä¿®æ¸ˆã€‘è¨˜äº‹æœ¬æ–‡ã®å–å¾—ã«ãŠã„ã¦ã€è¤‡æ•°ãƒšãƒ¼ã‚¸å·¡å›ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€**1ãƒšãƒ¼ã‚¸ç›®ã®ã¿**ã‚’å–å¾—ã€‚404ãªã©ã§å–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚
4. å…¨è¨˜äº‹ã‚’æŠ•ç¨¿æ—¥ã®æ–°ã—ã„é †ã«ä¸¦ã³æ›¿ãˆ (A-Dåˆ—ã‚’åŸºæº–ã«ã‚½ãƒ¼ãƒˆ)ã€‚
Â  Â  -> ã‚½ãƒ¼ãƒˆç›´å‰ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§Cåˆ—ã®æ›œæ—¥ã‚’**å€‹åˆ¥ findReplace ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å‰Šé™¤**ã€‚
Â  Â  -> ã€æ”¹ä¿®å¾Œã€‘ã‚½ãƒ¼ãƒˆç›´å‰ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§Cåˆ—ã®è¡¨ç¤ºå½¢å¼ã‚’**æ—¥æ™‚(yyyy/mm/dd hh:mm:ss)ã«è¨­å®š**ã€‚
Â  Â  -> ã€æ”¹ä¿®å¾Œã€‘ã‚½ãƒ¼ãƒˆã‚’Pythonãƒ¡ãƒ¢ãƒªã‹ã‚‰**ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆAPIã«ã‚ˆã‚‹ã‚½ãƒ¼ãƒˆ**ã«åˆ‡ã‚Šæ›¿ãˆã€‚
5. ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè¨˜äº‹ã«å¯¾ã—ã€æ–°ã—ã„ã‚‚ã®ã‹ã‚‰Geminiåˆ†æï¼ˆG, H, Iåˆ—ï¼‰ã‚’å®Ÿè¡Œã€‚
Â  Â  Geminiåˆ†æã§ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ã€ãã“ã§å‡¦ç†ã‚’ä¸­æ–­ã™ã‚‹ã€‚
"""

import os
import json
import time
import re
import random
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional, Set, Dict, Any
import sys
from urllib.parse import urlparse, parse_qs, urlunparse, urlencode # è¿½åŠ 

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# --- Gemini API é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from google import genai
from google.genai import types
from google.api_core.exceptions import ResourceExhausted
# ------------------------------------

# ====== è¨­å®š ======
SHARED_SPREADSHEET_ID = "1Ru2DT_zzKjTJptchWJitCb67VoffImGhgeOVjwlKukc"
KEYWORD_FILE = "keywords.txt"
SOURCE_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
SOURCE_SHEET_NAME = "Yahoo"
DEST_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
# æ›œæ—¥å‰Šé™¤ã®å¯¾è±¡ã¨ã™ã‚‹æœ€å¤§è¡Œæ•°ã‚’10000ã«è¨­å®š
MAX_SHEET_ROWS_FOR_REPLACE = 10000
MAX_PAGES = 10 # è¨˜äº‹æœ¬æ–‡å–å¾—ã®æœ€å¤§å·¡å›ãƒšãƒ¼ã‚¸æ•° (â€»ãƒ­ã‚¸ãƒƒã‚¯æ”¹ä¿®ã«ã‚ˆã‚Šç¾åœ¨ã¯1ãƒšãƒ¼ã‚¸ã®ã¿å–å¾—)

YAHOO_SHEET_HEADERS = ["URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥æ™‚", "ã‚½ãƒ¼ã‚¹", "æœ¬æ–‡", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "å¯¾è±¡ä¼æ¥­", "ã‚«ãƒ†ã‚´ãƒªåˆ†é¡", "ãƒã‚¸ãƒã‚¬åˆ†é¡"]
REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}
TZ_JST = timezone(timedelta(hours=9))

PROMPT_FILES = [
    "prompt_gemini_role.txt",
    "prompt_posinega.txt",
    "prompt_category.txt",
    "prompt_target_company.txt"
]

try:
    GEMINI_CLIENT = genai.Client()
except Exception as e:
    print(f"è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    GEMINI_CLIENT = None

GEMINI_PROMPT_TEMPLATE = None

# ====== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ ======

def jst_now() -> datetime:
    return datetime.now(TZ_JST)

def format_datetime(dt_obj) -> str:
    # ã€ä¿®æ­£ç‚¹â‘ ã€‘æ—¥æ™‚ã®è¡¨ç¤ºå½¢å¼ã‚’ yyyy/mm/dd hh:mm:ss ã«å¤‰æ›´
    return dt_obj.strftime("%Y/%m/%d %H:%M:%S") # 2025/10/08 10:00:28 ã®å½¢å¼

def parse_post_date(raw, today_jst: datetime) -> Optional[datetime]:
    if raw is None: return None
    if isinstance(raw, str):
        s = raw.strip()
        
        # æ›œæ—¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‰Šé™¤ã™ã‚‹æ­£è¦è¡¨ç¾ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
        s = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", s).strip()
        
        # é…ä¿¡ã¨ã„ã†æ–‡å­—ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤
        s = s.replace('é…ä¿¡', '').strip()
        
        # ä¿®æ­£å¾Œã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å«ã‚ã¦ãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
        for fmt in ("%Y/%m/%d %H:%M:%S", "%y/%m/%d %H:%M", "%m/%d %H:%M", "%Y/%m/%d %H:%M"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m/%d %H:%M":
                    # å¹´ãŒãªã„å½¢å¼ã®å ´åˆã€ä»Šå¹´ã‚’é©ç”¨
                    dt = dt.replace(year=today_jst.year)
                
                # å¹´ãŒæœªæ¥ï¼ˆç¾åœ¨æœˆã®ç¿Œæœˆä»¥é™ï¼‰ã§ã‚ã‚Œã°ã€å‰å¹´ã«ä¿®æ­£ã™ã‚‹ (æœˆæ—¥ã®ã¿ã®å½¢å¼ã‚’è€ƒæ…®)
                if dt.replace(tzinfo=TZ_JST) > today_jst + timedelta(days=31):
                    dt = dt.replace(year=dt.year - 1)
                    
                return dt.replace(tzinfo=TZ_JST)
            except ValueError:
                pass
        return None

def build_gspread_client() -> gspread.Client:
    try:
        # ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
        creds_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
            return gspread.authorize(credentials)
        else:
            # GCP_SERVICE_ACCOUNT_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«èªè¨¼ã‚’è©¦ã¿ã‚‹ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
            try:
                return gspread.service_account(filename='credentials.json')
            except FileNotFoundError:
                raise RuntimeError("Googleèªè¨¼æƒ…å ± (GCP_SERVICE_ACCOUNT_KEY)ãŒç’°å¢ƒå¤‰æ•°ã€ã¾ãŸã¯ 'credentials.json' ãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")

def load_keywords(filename: str) -> List[str]:
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            keywords = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        if not keywords:
            raise ValueError("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã«æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return keywords
    except FileNotFoundError:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []
    except Exception as e:
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def load_gemini_prompt() -> str:
    global GEMINI_PROMPT_TEMPLATE
    if GEMINI_PROMPT_TEMPLATE is not None:
        return GEMINI_PROMPT_TEMPLATE
        
    combined_instructions = []
    
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        role_instruction = ""

        role_file = PROMPT_FILES[0]
        file_path = os.path.join(script_dir, role_file)
        with open(file_path, 'r', encoding='utf-8') as f:
            role_instruction = f.read().strip()
        
        for filename in PROMPT_FILES[1:]:
            file_path = os.path.join(script_dir, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            if content:
                combined_instructions.append(content)
                        
        if not role_instruction or not combined_instructions:
            print("è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒä¸å®Œå…¨ã¾ãŸã¯ç©ºã§ã™ã€‚")
            return ""

        base_prompt = role_instruction + "\n" + "\n".join(combined_instructions)
        base_prompt += "\n\nè¨˜äº‹æœ¬æ–‡:\n{TEXT_TO_ANALYZE}"

        GEMINI_PROMPT_TEMPLATE = base_prompt
        print(f" Geminiãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ {PROMPT_FILES} ã‹ã‚‰èª­ã¿è¾¼ã¿ã€çµåˆã—ã¾ã—ãŸã€‚")
        return base_prompt
        
    except FileNotFoundError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å: {e.filename}")
        return ""
    except Exception as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return ""

def request_with_retry(url: str, max_retries: int = 3) -> Optional[requests.Response]:
    """ è¨˜äº‹æœ¬æ–‡å–å¾—ç”¨ã®ãƒªãƒˆãƒ©ã‚¤ä»˜ããƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒ«ãƒ‘ãƒ¼ """
    for attempt in range(max_retries):
        try:
            res = requests.get(url, headers=REQ_HEADERS, timeout=20)
            
            # ğŸ’¡ æ”¹ä¿®ç‚¹â‘¡: 404 Client Error ã®å ´åˆã€ãƒªãƒˆãƒ©ã‚¤ã›ãš None ã‚’è¿”ã—ã¦å³åº§ã«ã‚¹ã‚­ãƒƒãƒ—
            if res.status_code == 404:
                print(f"  âŒ ãƒšãƒ¼ã‚¸ãªã— (404 Client Error): {url}")
                return None
                
            res.raise_for_status()
            return res
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt + random.random()
                print(f"  âš ï¸ æ¥ç¶šã‚¨ãƒ©ãƒ¼ã€ãƒªãƒˆãƒ©ã‚¤ä¸­... ({attempt + 1}/{max_retries})ã€‚å¾…æ©Ÿ: {wait_time:.2f}ç§’")
                time.sleep(wait_time)
            else:
                print(f"  âŒ æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤å¤±æ•—: {e}")
                return None
    return None

# ====== Gemini åˆ†æé–¢æ•° (å¤‰æ›´ãªã—) ======
def analyze_with_gemini(text_to_analyze: str) -> Tuple[str, str, str, bool]:
    if not GEMINI_CLIENT:
        return "N/A", "N/A", "N/A", False
        
    if not text_to_analyze.strip():
        return "N/A", "N/A", "N/A", False

    prompt_template = load_gemini_prompt()
    if not prompt_template:
        return "ERROR(Prompt Missing)", "ERROR", "ERROR", False

    MAX_RETRIES = 3
    MAX_CHARACTERS = 15000
    
    for attempt in range(MAX_RETRIES):
        try:
            text_for_prompt = text_to_analyze[:MAX_CHARACTERS]
            prompt = prompt_template.replace("{TEXT_TO_ANALYZE}", text_for_prompt)
            
            response = GEMINI_CLIENT.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema={"type": "object", "properties": {
                        "company_info": {"type": "string", "description": "è¨˜äº‹ã®ä¸»é¡Œä¼æ¥­åã¨ï¼ˆï¼‰å†…ã«å…±åŒé–‹ç™ºä¼æ¥­åã‚’è¨˜è¼‰ã—ãŸçµæœ"},
                        "category": {"type": "string", "description": "ä¼æ¥­ã€ãƒ¢ãƒ‡ãƒ«ã€æŠ€è¡“ãªã©ã®åˆ†é¡çµæœ"},
                        "sentiment": {"type": "string", "description": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã®ã„ãšã‚Œã‹"}
                    }}
                ),
            )

            analysis = json.loads(response.text.strip())
            
            company_info = analysis.get("company_info", "N/A")
            category = analysis.get("category", "N/A")
            sentiment = analysis.get("sentiment", "N/A")

            return company_info, category, sentiment, False

        # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã‚’æœ€å„ªå…ˆã§æ•æ‰ã—ã€å¼·åˆ¶çµ‚äº†
        except ResourceExhausted as e:
            print(f"  ğŸš¨ Gemini API ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
            print("\n===== ğŸ›‘ ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚’æ¤œå‡ºã—ãŸãŸã‚ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’ç›´ã¡ã«ä¸­æ–­ã—ã¾ã™ã€‚ =====")
            sys.stdout.flush()
            sys.exit(1) # ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’çµ‚äº†

        # ã‚¯ã‚©ãƒ¼ã‚¿ä»¥å¤–ã®ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã®ã¿ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã¨ã™ã‚‹
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                wait_time = 2 ** attempt + random.random()
                print(f"  âš ï¸ Gemini API ä¸€æ™‚çš„ãªæ¥ç¶šã¾ãŸã¯å‡¦ç†ã‚¨ãƒ©ãƒ¼ã€‚{wait_time:.2f} ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})ã€‚")
                time.sleep(wait_time)
                continue
            else:
                print(f"Geminiåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                return "ERROR", "ERROR", "ERROR", False
        
    return "ERROR", "ERROR", "ERROR", False

# ====== ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° (ã‚½ãƒ¼ã‚¹æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£) ======

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword})...")
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={REQ_HEADERS['User-Agent']}")
    
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    try:
        driver_path = ChromeDriverManager().install()
        service = Service(driver_path)
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f" WebDriverã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []
        
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(search_url)
    
    try:
        WebDriverWait(driver, 20).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "li[class*='sc-1u4589e-0']"))
        )
        time.sleep(3)
    except Exception as e:
        print(f"  âš ï¸ ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯è¦ç´ æ¤œç´¢ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        time.sleep(5)
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()
    
    # è¨˜äº‹ãƒªã‚¹ãƒˆã®è¦ªè¦ç´ ã‚’ç‰¹å®š (ã‚»ãƒ¬ã‚¯ã‚¿ã¯é©å®œèª¿æ•´ãŒå¿…è¦ã«ãªã‚‹å ´åˆãŒã‚ã‚‹)
    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    
    articles_data = []
    today_jst = jst_now()
    
    for article in articles:
        try:
            # A. ã‚¿ã‚¤ãƒˆãƒ«
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            
            # B. URL
            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag and link_tag["href"].startswith("https://news.yahoo.co.jp/articles/") else ""
            
            # C. æŠ•ç¨¿æ—¥æ™‚ (Cåˆ—) æŠ½å‡º
            date_str = ""
            time_tag = article.find("time")
            if time_tag:
                date_str = time_tag.text.strip()
            
            # D. ã‚½ãƒ¼ã‚¹ (Dåˆ—) æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„
            source_text = ""
            source_container = article.find("div", class_=re.compile("sc-n3vj8g-0"))
            
            if source_container:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚„ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å¾Œã«ç¶šãæœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
                time_and_comments = source_container.find("div", class_=re.compile("sc-110wjhy-8"))
                
                if time_and_comments:
                    # divå†…ã®å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€æ—¥ä»˜ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã®è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
                    source_candidates = [
                        span.text.strip() for span in time_and_comments.find_all("span")
                        if not span.find("svg") # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¤ã‚³ãƒ³ã§ã¯ãªã„
                        and not re.match(r'\d{1,2}/\d{1,2}\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)\d{1,2}:\d{2}', span.text.strip()) # æ—¥ä»˜ã§ã¯ãªã„
                    ]
                    # æœ€ã‚‚é•·ã„ï¼ˆã‚½ãƒ¼ã‚¹ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ï¼‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¡ç”¨
                    if source_candidates:
                        source_text = max(source_candidates, key=len)
                        
                    # ä¸Šè¨˜ã§å–å¾—ã§ããªã„å ´åˆã€ç›´ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’æ¢ã™
                    if not source_text:
                        for content in time_and_comments.contents:
                            if content.name is None and content.strip() and not re.match(r'\d{1,2}/\d{1,2}\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)\d{1,2}:\d{2}', content.strip()):
                                source_text = content.strip()
                                break
                    
            if title and url:
                formatted_date = ""
                if date_str:
                    try:
                        # å–å¾—ã—ãŸç”Ÿã®æ—¥ä»˜æ–‡å­—åˆ—ã‹ã‚‰æ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                        dt_obj = parse_post_date(date_str, today_jst)
                        
                        if dt_obj:
                            # ä¿®æ­£ã—ãŸ format_datetime ã‚’ä½¿ç”¨ã—ã€yyyy/mm/dd hh:mm:ss å½¢å¼ã§æ ¼ç´
                            formatted_date = format_datetime(dt_obj)
                        else:
                            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯æ›œæ—¥ã ã‘å‰Šé™¤ã—ãŸç”Ÿæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä¿æŒ
                            formatted_date = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", date_str).strip()
                    except:
                        formatted_date = date_str

                articles_data.append({
                    "URL": url,
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "æŠ•ç¨¿æ—¥æ™‚": formatted_date if formatted_date else "å–å¾—ä¸å¯",
                    "ã‚½ãƒ¼ã‚¹": source_text if source_text else "å–å¾—ä¸å¯"
                })
        except Exception as e:
            continue
            
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(articles_data)} ä»¶å–å¾—")
    return articles_data

# ====== è©³ç´°å–å¾—é–¢æ•° (è¤‡æ•°ãƒšãƒ¼ã‚¸å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€1ãƒšãƒ¼ã‚¸ç›®ã®ã¿å–å¾—ã«ä¿®æ­£) ======
def fetch_article_body_and_comments(base_url: str) -> Tuple[str, int, Optional[str]]:
    """
    è¨˜äº‹IDãƒ™ãƒ¼ã‚¹ã® '?page=N' ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸè¤‡æ•°ãƒšãƒ¼ã‚¸å·¡å›ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€
    1ãƒšãƒ¼ã‚¸ç›®ã®ã¿ã®å–å¾—ã«ä¿®æ­£ã€‚
    """
    comment_count = -1 # ğŸ’¡ æ”¹ä¿®ç‚¹â‘ : ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãŒå–å¾—ã§ããªã„å ´åˆã¯ -1 (æœªå–å¾—)ã¨ã—ã¦ãƒãƒ¼ã‚¯
    extracted_date_str = None
    
    # URLã‹ã‚‰è¨˜äº‹IDã‚’å–å¾— (ä¾‹: aaa7c40ed1706ff109ad5e48ccebbfe598805ffd)
    article_id_match = re.search(r'/articles/([a-f0-9]+)', base_url)
    if not article_id_match:
        print(f"  âŒ URLã‹ã‚‰è¨˜äº‹IDãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {base_url}")
        return "æœ¬æ–‡å–å¾—ä¸å¯", -1, None
        
    # å¸¸ã«1ãƒšãƒ¼ã‚¸ç›®ï¼ˆãƒ™ãƒ¼ã‚¹URLï¼‰ã®ã¿ã‚’å–å¾—
    current_url = base_url.split('?')[0] # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ãƒ™ãƒ¼ã‚¹URLã‚’ç¢ºä¿
    
    # 2. HTMLå–å¾—ã¨BeautifulSoupã®åˆæœŸåŒ–
    response = request_with_retry(current_url)
    
    if not response:
        # ğŸ’¡ æ”¹ä¿®ç‚¹â‘¡: request_with_retryã§ãƒªãƒˆãƒ©ã‚¤å¾Œã‚‚å–å¾—ã§ããªã‹ã£ãŸå ´åˆï¼ˆ404ã‚’å«ã‚€ï¼‰ã€ã‚¹ã‚­ãƒƒãƒ—
        print(f"  âŒ è¨˜äº‹æœ¬æ–‡ã®å–å¾—ã«å¤±æ•—ã—ãŸãŸã‚ã€æœ¬æ–‡å–å¾—ä¸å¯ã‚’è¿”ã—ã¾ã™ã€‚: {current_url}")
        return "æœ¬æ–‡å–å¾—ä¸å¯", -1, None
        
    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ 1 ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    soup = BeautifulSoup(response.text, 'html.parser')

    # 3. è¨˜äº‹æœ¬æ–‡ã®æŠ½å‡º (ãƒšãƒ¼ã‚¸1ã®ã¿)
    article_content = soup.find('article') or soup.find('div', class_='article_body') or soup.find('div', class_=re.compile(r'article_detail|article_body'))

    current_body = []
    if article_content:
        # æœ€æ–°ã®HTMLæ§‹é€ ã«å¯¾å¿œã—ãŸã‚»ãƒ¬ã‚¯ã‚¿
        paragraphs = article_content.find_all('p', class_=re.compile(r'sc-\w+-0\s+\w+.*highLightSearchTarget'))
        if not paragraphs: # ä¸Šè¨˜ã‚»ãƒ¬ã‚¯ã‚¿ã§å–å¾—ã§ããªã‘ã‚Œã°æ±ç”¨<p>ã‚’è©¦ã™
            paragraphs = article_content.find_all('p')
            
        for p in paragraphs:
            text = p.get_text(strip=True)
            if text:
                current_body.append(text)
    
    # 4. æœ¬æ–‡ã‚’çµåˆ
    body_text = "\n".join(current_body)
    
    # --- ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¨æ—¥æ™‚ ---
    
    # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’è¡¨ã™ãƒœã‚¿ãƒ³ã¾ãŸã¯ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    comment_button = soup.find("button", attrs={"data-cl-params": re.compile(r"cmtmod")}) or \
                         soup.find("a", attrs={"data-cl-params": re.compile(r"cmtmod")})
    if comment_button:
        # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å«ã‚€è¦ç´ ã‹ã‚‰æ•°å­—ã‚’æŠ½å‡º
        text = comment_button.get_text(strip=True).replace(",", "")
        match = re.search(r'(\d+)', text)
        if match:
            comment_count = int(match.group(1)) # 0ä»¥ä¸Šã®å€¤

    # Cåˆ—è£œå®Œç”¨ã®æ—¥æ™‚ã‚’æœ¬æ–‡ã®å†’é ­ã‹ã‚‰æŠ½å‡ºï¼ˆã€Œ10/20(æœˆ) 15:30é…ä¿¡ã€å½¢å¼ï¼‰
    if body_text:
        body_text_partial = "\n".join(body_text.split('\n')[:3])
        match = re.search(r'(\d{1,2}/\d{1,2})\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)(\s*)(\d{1,2}:\d{2})é…ä¿¡', body_text_partial)
        if match:
            month_day = match.group(1)
            time_str = match.group(3)
            # æ›œæ—¥ãƒ»é…ä¿¡ã‚’å‰Šé™¤ã—ãŸå½¢å¼ (ä¾‹: 10/20 15:30)
            extracted_date_str = f"{month_day} {time_str}"
            
    return body_text if body_text else "æœ¬æ–‡å–å¾—ä¸å¯", comment_count, extracted_date_str


# ====== ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ“ä½œé–¢æ•° (ã‚½ãƒ¼ãƒˆ/ç½®æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£) ======

def set_row_height(ws: gspread.Worksheet, row_height_pixels: int):
    try:
        requests = []
        requests.append({
           "updateDimensionProperties": {
                 "range": {
                     "sheetId": ws.id,
                     "dimension": "ROWS",
                     "startIndex": 1,
                     "endIndex": ws.row_count
                 },
                 "properties": {
                     "pixelSize": row_height_pixels
                 },
                 "fields": "pixelSize"
            }
        })
        ws.spreadsheet.batch_update({"requests": requests})
        print(f" 2è¡Œç›®ä»¥é™ã®**è¡Œã®é«˜ã•**ã‚’ {row_height_pixels} ãƒ”ã‚¯ã‚»ãƒ«ã«è¨­å®šã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f" âš ï¸ è¡Œé«˜è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")


def ensure_source_sheet_headers(sh: gspread.Spreadsheet) -> gspread.Worksheet:
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=SOURCE_SHEET_NAME, rows=str(MAX_SHEET_ROWS_FOR_REPLACE), cols=str(len(YAHOO_SHEET_HEADERS)))
        
    current_headers = ws.row_values(1)
    if current_headers != YAHOO_SHEET_HEADERS:
        ws.update(range_name=f'A1:{gspread.utils.rowcol_to_a1(1, len(YAHOO_SHEET_HEADERS))}', values=[YAHOO_SHEET_HEADERS])
    return ws

def write_news_list_to_source(gc: gspread.Client, articles: list[dict]):
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    worksheet = ensure_source_sheet_headers(sh)
            
    existing_data = worksheet.get_all_values(value_render_option='UNFORMATTED_VALUE')
    # æ—¢å­˜ã®Aåˆ—ï¼ˆURLï¼‰ã‚’ã‚»ãƒƒãƒˆã«æ ¼ç´
    existing_urls = set(str(row[0]) for row in existing_data[1:] if len(row) > 0 and str(row[0]).startswith("http"))
    
    # URLãŒé‡è¤‡ã—ãªã„æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
    new_data = [[a['URL'], a['ã‚¿ã‚¤ãƒˆãƒ«'], a['æŠ•ç¨¿æ—¥æ™‚'], a['ã‚½ãƒ¼ã‚¹']] for a in articles if a['URL'] not in existing_urls]
    
    if new_data:
        # Aï½Dåˆ—ã«è¿½è¨˜
        worksheet.append_rows(new_data, value_input_option='USER_ENTERED')
        print(f"  SOURCEã‚·ãƒ¼ãƒˆã« {len(new_data)} ä»¶è¿½è¨˜ã—ã¾ã—ãŸã€‚")
    else:
        print("  SOURCEã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

def sort_yahoo_sheet(gc: gspread.Client):
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        worksheet = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("ã‚½ãƒ¼ãƒˆã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # æœ€çµ‚è¡Œã‚’å–å¾—ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ç¯„å›²ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ï¼‰
    last_row = len(worksheet.col_values(1))
    
    if last_row <= 1:
        print("ã‚½ãƒ¼ãƒˆå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚½ãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    # --- ğŸš¨ æ›œæ—¥å‰Šé™¤ã®ãŸã‚ã® batch_update (æ—¢å­˜) ---
    try:
        requests = []
        
        # æ›œæ—¥ãƒªã‚¹ãƒˆ
        days_of_week = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
        
        # 1. å„æ›œæ—¥ã«å¯¾å¿œã™ã‚‹å€‹åˆ¥ã®ç½®æ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç”Ÿæˆ (7ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)
        for day in days_of_week:
            requests.append({
                "findReplace": {
                    "range": {
                        "sheetId": worksheet.id,
                        "startRowIndex": 1, # 2è¡Œç›®ã‹ã‚‰
                        "endRowIndex": MAX_SHEET_ROWS_FOR_REPLACE, # 10000è¡Œç›®ã¾ã§
                        "startColumnIndex": 2, # Cåˆ—
                        "endColumnIndex": 3 # Cåˆ—
                    },
                    "find": rf"\({day}\)", # f-stringã¨raw stringã§ \(æœˆ\) ã®æ­£è¦è¡¨ç¾ã‚’ç”Ÿæˆ
                    "replacement": "",
                    "searchByRegex": True,
                }
            })
            
        # 2. æ›œæ—¥ã®ç›´å¾Œã«æ®‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚„é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã€åŠè§’ã‚¹ãƒšãƒ¼ã‚¹1ã¤ã«çµ±ä¸€ (1ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)
        requests.append({
            "findReplace": {
                "range": {
                    "sheetId": worksheet.id,
                    "startRowIndex": 1,
                    "endRowIndex": MAX_SHEET_ROWS_FOR_REPLACE,
                    "startColumnIndex": 2,
                    "endColumnIndex": 3
                },
                "find": r"\s{2,}",
                "replacement": " ",
                "searchByRegex": True,
            }
        })
        
        # 3. æœ€å¾Œã«æ®‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å‰å¾Œã®ä¸è¦ãªç©ºç™½ã‚’å‰Šé™¤ (Trimæ©Ÿèƒ½ã®ä»£æ›¿ - 1ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)
        requests.append({
            "findReplace": {
                "range": {
                    "sheetId": worksheet.id,
                    "startRowIndex": 1,
                    "endRowIndex": MAX_SHEET_ROWS_FOR_REPLACE,
                    "startColumnIndex": 2,
                    "endColumnIndex": 3
                },
                "find": r"^\s+|\s+$",
                "replacement": "",
                "searchByRegex": True,
            }
        })
        
        # batch_update ã§ã¾ã¨ã‚ã¦å®Ÿè¡Œ
        worksheet.spreadsheet.batch_update({"requests": requests})
        print(" ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§Cåˆ—ã®**æ›œæ—¥è¨˜è¼‰ã‚’å€‹åˆ¥ã«å‰Šé™¤ã—ã€ä½“è£ã‚’æ•´ãˆã¾ã—ãŸ**ã€‚")
        
    except Exception as e:
        print(f" âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã®ç½®æ›ã‚¨ãƒ©ãƒ¼: {e}")
    # ----------------------------------------------------

    # --- ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆâ‘¡ã€‘æ—¥æ™‚ã®è¡¨ç¤ºå½¢å¼å¤‰æ›´ ---
    try:
        format_requests = []
        # C2ã‹ã‚‰C[last_row]ã®ç¯„å›²
        format_requests.append({
            "updateCells": {
                "range": {
                    "sheetId": worksheet.id,
                    "startRowIndex": 1, # 2è¡Œç›® (ãƒ‡ãƒ¼ã‚¿é–‹å§‹)
                    "endRowIndex": last_row,
                    "startColumnIndex": 2, # Cåˆ— (3åˆ—ç›®)
                    "endColumnIndex": 3
                },
                "cell": {
                    "userEnteredFormat": {
                        "numberFormat": {
                            "type": "DATE_TIME",
                            "pattern": "yyyy/mm/dd hh:mm:ss"
                        }
                    }
                },
                "fields": "userEnteredFormat.numberFormat"
            }
        })
        
        worksheet.spreadsheet.batch_update({"requests": format_requests})
        print(f" âœ… Cåˆ—(2è¡Œç›®ã€œ{last_row}è¡Œ) ã®è¡¨ç¤ºå½¢å¼ã‚’ 'yyyy/mm/dd hh:mm:ss' ã«è¨­å®šã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f" âš ï¸ Cåˆ—ã®è¡¨ç¤ºå½¢å¼è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")

    # --- ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆâ‘ ã€‘ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§ã®ã‚½ãƒ¼ãƒˆ (APIã‚½ãƒ¼ãƒˆ) ---
    try:
        last_col_index = len(YAHOO_SHEET_HEADERS) # 9 (Iåˆ—)
        last_col_a1 = gspread.utils.col_to_letter(last_col_index)
        sort_range = f'A2:{last_col_a1}{last_row}'

        # Cåˆ—ï¼ˆ3åˆ—ç›®ï¼‰ã‚’é™é †ï¼ˆæ–°ã—ã„é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ
        # gspreadã®sortãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        worksheet.sort((3, 'desc'), range=sort_range)
        print(" âœ… SOURCEã‚·ãƒ¼ãƒˆã‚’æŠ•ç¨¿æ—¥æ™‚ã®**æ–°ã—ã„é †**ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")
    except Exception as e:
        print(f" âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã®ã‚½ãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

# ====== æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ã¨å³æ™‚æ›´æ–° (E, Fåˆ—) (ãƒ­ã‚¸ãƒƒã‚¯åæ˜ æ¸ˆã¿) ======

def fetch_details_and_update_sheet(gc: gspread.Client):
    """ 
    Eåˆ—, Fåˆ—ãŒæœªå…¥åŠ›ã®è¡Œã«å¯¾ã—ã€è©³ç´°å–å¾—ã¨Cåˆ—ã®æ—¥ä»˜è£œå®Œã‚’è¡Œã„ã€è¡Œã”ã¨ã«å³æ™‚æ›´æ–°ã™ã‚‹ã€‚
    ğŸ’¡ æ”¹ä¿®ç‚¹: 
        1. æœ¬æ–‡å–å¾—ã¯åˆå›ã®ã¿ã€‚
        2. æœ¬æ–‡å–å¾—æ¸ˆ ã‹ã¤ 3æ—¥ä»¥å†…ã®è¨˜äº‹ã¯ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®ã¿æ›´æ–°ã€‚
        3. æœ¬æ–‡å–å¾—æ¸ˆ ã‹ã¤ 3æ—¥ã‚ˆã‚Šå¤ã„è¨˜äº‹ã¯ã€å®Œå…¨ã«ã‚¹ã‚­ãƒƒãƒ—ã€‚
    """
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("è©³ç´°å–å¾—ã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    all_values = ws.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€è©³ç´°å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
        
    data_rows = all_values[1:]
    update_count = 0
    
    print("\n===== ğŸ“„ ã‚¹ãƒ†ãƒƒãƒ—â‘¡ è¨˜äº‹æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ãƒ»å³æ™‚åæ˜  (E, Fåˆ—) =====")

    now_jst = jst_now()
    # å¢ƒç•Œç·šã®è¨­å®š: ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œæ—¥ã‹ã‚‰3æ—¥å‰ã®00:00:00ã‚’è¨ˆç®—
    three_days_ago = (now_jst - timedelta(days=3)).replace(hour=0, minute=0, second=0, microsecond=0)

    for idx, data_row in enumerate(data_rows):
        # è¡Œã®é•·ã•ã‚’ç¢ºèªã—ã€YAHOO_SHEET_HEADERS ã®æ•°ã«åˆã‚ã›ã¦åŸ‹ã‚ã‚‹
        if len(data_row) < len(YAHOO_SHEET_HEADERS):
            data_row.extend([''] * (len(YAHOO_SHEET_HEADERS) - len(data_row)))
            
        row_num = idx + 2
        
        url = str(data_row[0])
        title = str(data_row[1])
        post_date_raw = str(data_row[2]) # Cåˆ—
        source = str(data_row[3])        # Dåˆ—
        body = str(data_row[4])          # Eåˆ—
        comment_count_str = str(data_row[5]) # Fåˆ—
        
        if not url.strip() or not url.startswith('http'):
            print(f"  - è¡Œ {row_num}: URLãŒç„¡åŠ¹ãªãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue

        is_content_fetched = (body.strip() and body != "æœ¬æ–‡å–å¾—ä¸å¯") # æœ¬æ–‡ãŒå–å¾—æ¸ˆã¿ã‹ã©ã†ã‹
        needs_body_fetch = not is_content_fetched # æœ¬æ–‡å–å¾—ãŒåˆå›å¿…è¦ã‹ã©ã†ã‹
        
        post_date_dt = parse_post_date(post_date_raw, now_jst)

        # æŠ•ç¨¿æ—¥æ™‚ãŒ3æ—¥ä»¥å†…ã§ã‚ã‚‹ã‹
        is_within_three_days = (post_date_dt and post_date_dt >= three_days_ago)
        
        
        # --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ ---
        
        # 1. ã€å®Œå…¨ã‚¹ã‚­ãƒƒãƒ—ã€‘ æœ¬æ–‡å–å¾—æ¸ˆã¿ ã‹ã¤ 3æ—¥ã‚ˆã‚Šå¤ã„è¨˜äº‹
        if is_content_fetched and not is_within_three_days:
            print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): æœ¬æ–‡å–å¾—æ¸ˆã¿ã‹ã¤3æ—¥ã‚ˆã‚Šå¤ã„è¨˜äº‹ã®ãŸã‚ã€**å®Œå…¨ã‚¹ã‚­ãƒƒãƒ—**ã€‚")
            continue
            
        # 2. ã€ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿æ›´æ–°ã€‘ æœ¬æ–‡å–å¾—æ¸ˆã¿ ã‹ã¤ 3æ—¥ä»¥å†… ã®è¨˜äº‹
        is_comment_only_update = is_content_fetched and is_within_three_days
        
        # 3. ã€å®Œå…¨æ›´æ–°ã€‘ æœ¬æ–‡æœªå–å¾—ã®è¨˜äº‹ (3æ—¥ä»¥å†…/å¤–ã«é–¢ã‚ã‚‰ãšæœ¬æ–‡å–å¾—ã‚’è©¦ã¿ã‚‹)
        needs_full_fetch = needs_body_fetch
        
        # 4. è©³ç´°å–å¾—ã®å®Ÿè¡ŒãŒå¿…è¦ãªå ´åˆ: 2 ã¾ãŸã¯ 3 ã®ã„ãšã‚Œã‹
        needs_detail_fetch = is_comment_only_update or needs_full_fetch

        if not needs_detail_fetch:
            print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): è©³ç´°æ›´æ–°ã®å¿…è¦ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue


        # --- è©³ç´°å–å¾—ã‚’å®Ÿè¡Œ ---
        if needs_full_fetch:
            print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): **æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆæ•°/æ—¥æ™‚è£œå®Œã‚’å–å¾—ä¸­... (å®Œå…¨å–å¾—)**")
        elif is_comment_only_update:
            print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): **ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’æ›´æ–°ä¸­... (è»½é‡æ›´æ–°)**")
            
        fetched_body, fetched_comment_count, extracted_date = fetch_article_body_and_comments(url)

        new_body = body
        new_comment_count = comment_count_str
        new_post_date = post_date_raw
        
        needs_update_to_sheet = False

        # 1. Eåˆ—(æœ¬æ–‡)ã®æ›´æ–° (æœ¬æ–‡æœªå–å¾—ã®å ´åˆã®ã¿)
        if needs_full_fetch:
            if fetched_body != "æœ¬æ–‡å–å¾—ä¸å¯":
                if new_body != fetched_body:
                    new_body = fetched_body
                    needs_update_to_sheet = True
            elif body != "æœ¬æ–‡å–å¾—ä¸å¯": # ä»¥å‰æˆåŠŸã—ãŸæœ¬æ–‡ãŒä¸Šæ›¸ãã•ã‚Œãªã„ã‚ˆã†ã«ã€æœ¬æ–‡å–å¾—ä¸å¯ã«ãªã£ãŸå ´åˆã®ã¿æ›´æ–°
                 new_body = "æœ¬æ–‡å–å¾—ä¸å¯"
                 needs_update_to_sheet = True
        elif is_comment_only_update and fetched_body == "æœ¬æ–‡å–å¾—ä¸å¯":
            # ã‚³ãƒ¡ãƒ³ãƒˆæ›´æ–°ç›®çš„ã§APIã‚’å©ã„ãŸãŒã€è¨˜äº‹ãŒ404ãªã©ã§æ¶ˆãˆã¦ã„ãŸå ´åˆã€Eåˆ—ã‚’æ›´æ–°
             if body != "æœ¬æ–‡å–å¾—ä¸å¯":
                 new_body = "æœ¬æ–‡å–å¾—ä¸å¯"
                 needs_update_to_sheet = True
            
        # 2. Cåˆ—(æ—¥æ™‚)ã®æ›´æ–° (æœ¬æ–‡æœªå–å¾—ã®å ´åˆã€ã¾ãŸã¯æ—¥ä»˜ãŒç©ºã®å ´åˆã®ã¿)
        if needs_full_fetch and ("å–å¾—ä¸å¯" in post_date_raw or not post_date_raw.strip()) and extracted_date:
            dt_obj = parse_post_date(extracted_date, now_jst)
            if dt_obj:
                # format_datetimeã§ yyyy/mm/dd hh:mm:ss å½¢å¼ã«å¤‰æ›
                formatted_dt = format_datetime(dt_obj)
                if formatted_dt != post_date_raw:
                    new_post_date = formatted_dt
                    needs_update_to_sheet = True
            else:
                raw_date = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", extracted_date).strip()
                if raw_date != post_date_raw:
                    new_post_date = raw_date
                    needs_update_to_sheet = True
            
        # 3. Fåˆ—(ã‚³ãƒ¡ãƒ³ãƒˆæ•°)ã®æ›´æ–°
        if fetched_comment_count != -1:
            # needs_full_fetch=True (åˆå›å–å¾—) ã¾ãŸã¯ is_comment_only_update=True (3æ—¥ä»¥å†…ã‹ã¤æœ¬æ–‡å–å¾—æ¸ˆ) ã®å ´åˆ
            if needs_full_fetch or is_comment_only_update:
                if str(fetched_comment_count) != comment_count_str:
                    new_comment_count = str(fetched_comment_count)
                    needs_update_to_sheet = True
        else:
            if needs_detail_fetch: # å–å¾—ã‚’è©¦ã¿ãŸå ´åˆã®ã¿ãƒ­ã‚°å‡ºåŠ›
                print(f"    - âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ—¢å­˜ã®å€¤ ({comment_count_str}) ã‚’ç¶­æŒã—ã¾ã™ã€‚")


        if needs_update_to_sheet:
            # C, D, E, Fåˆ—ã‚’å³æ™‚æ›´æ–° (Dåˆ—ã¯ã‚½ãƒ¼ã‚¹ã€‚æœ¬æ–‡å–å¾—ã§ã¯æ›´æ–°ã•ã‚Œãªã„ãŒã€æ›´æ–°ç¯„å›²ã«å«ã‚ã‚‹)
            # C: new_post_date, D: source (å¤‰æ›´ãªã—), E: new_body, F: new_comment_count
            ws.update(
                range_name=f'C{row_num}:F{row_num}',
                values=[[new_post_date, source, new_body, new_comment_count]],
                value_input_option='USER_ENTERED'
            )
            update_count += 1
            time.sleep(1 + random.random() * 0.5)

    print(f" âœ… æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆæ•°å–å¾—ã¨æ—¥æ™‚è£œå®Œã‚’ {update_count} è¡Œã«ã¤ã„ã¦å®Ÿè¡Œã—ã€å³æ™‚åæ˜ ã—ã¾ã—ãŸã€‚")


# ====== Geminiåˆ†æã®å®Ÿè¡Œã¨å¼·åˆ¶ä¸­æ–­ (G, H, Iåˆ—) (å¤‰æ›´ãªã—) ======

def analyze_with_gemini_and_update_sheet(gc: gspread.Client):
    """ Gåˆ—, Håˆ—, Iåˆ—ãŒæœªå…¥åŠ›ã®è¡Œã«å¯¾ã—ã€Geminiåˆ†æã‚’è¡Œã„ã€åˆ†æçµæœã‚’å³æ™‚æ›´æ–°ã™ã‚‹ """
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("Geminiåˆ†æã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    all_values = ws.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€Geminiåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
        
    data_rows = all_values[1:]
    update_count = 0
    
    print("\n===== ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—â‘£ Geminiåˆ†æã®å®Ÿè¡Œãƒ»å³æ™‚åæ˜  (G, H, Iåˆ—) =====")

    for idx, data_row in enumerate(data_rows):
        # è¡Œã®é•·ã•ã‚’ç¢ºèªã—ã€YAHOO_SHEET_HEADERS ã®æ•°ã«åˆã‚ã›ã¦åŸ‹ã‚ã‚‹
        if len(data_row) < len(YAHOO_SHEET_HEADERS):
            data_row.extend([''] * (len(YAHOO_SHEET_HEADERS) - len(data_row)))
            
        row_num = idx + 2
        
        url = str(data_row[0])
        title = str(data_row[1])
        body = str(data_row[4])       # Eåˆ—
        company_info = str(data_row[6]) # Gåˆ—
        category = str(data_row[7])     # Håˆ—
        sentiment = str(data_row[8])    # Iåˆ—

        needs_analysis = not company_info.strip() or not category.strip() or not sentiment.strip()

        if not needs_analysis:
            continue
            
        if not body.strip() or body == "æœ¬æ–‡å–å¾—ä¸å¯":
            print(f"  - è¡Œ {row_num}: æœ¬æ–‡ãŒãªã„ãŸã‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€N/Aã‚’è¨­å®šã€‚")
            
            ws.update(
                range_name=f'G{row_num}:I{row_num}',
                values=[['N/A(No Body)', 'N/A', 'N/A']],
                value_input_option='USER_ENTERED'
            )
            update_count += 1
            time.sleep(1)
            continue
            
        if not url.strip():
            print(f"  - è¡Œ {row_num}: URLãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue

        print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): Geminiåˆ†æã‚’å®Ÿè¡Œä¸­...")

        # --- Geminiåˆ†æã‚’å®Ÿè¡Œ (G, H, Iåˆ—) ---
        final_company_info, final_category, final_sentiment, _ = analyze_with_gemini(body)
        
        ws.update(
            range_name=f'G{row_num}:I{row_num}',
            values=[[final_company_info, final_category, final_sentiment]],
            value_input_option='USER_ENTERED'
        )
        update_count += 1
        time.sleep(1 + random.random() * 0.5)

    print(f" âœ… Geminiåˆ†æã‚’ {update_count} è¡Œã«ã¤ã„ã¦å®Ÿè¡Œã—ã€å³æ™‚åæ˜ ã—ã¾ã—ãŸã€‚")


# ====== ãƒ¡ã‚¤ãƒ³å‡¦ç† (å¤‰æ›´ãªã—) ======

def main():
    print("--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ ---")
    
    keywords = load_keywords(KEYWORD_FILE)
    if not keywords:
        sys.exit(0)

    try:
        gc = build_gspread_client()
    except RuntimeError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)
    
    # â‘  ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—: Aï½Dåˆ—ã®å–å¾—ãƒ»è¿½è¨˜ã‚’å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
    for current_keyword in keywords:
        print(f"\n===== ğŸ”‘ ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—: {current_keyword} =====")
        yahoo_news_articles = get_yahoo_news_with_selenium(current_keyword)
        write_news_list_to_source(gc, yahoo_news_articles)
        time.sleep(2) # ã‚·ãƒ¼ãƒˆã¸ã®é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹å›é¿

    # â‘¡ ã‚¹ãƒ†ãƒƒãƒ—â‘¡ æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ã¨å³æ™‚æ›´æ–° (E, Fåˆ—)
    fetch_details_and_update_sheet(gc)

    # â‘¢ ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ã‚½ãƒ¼ãƒˆã¨Cåˆ—ã®æ•´å½¢ãƒ»æ›¸å¼è¨­å®š
    print("\n===== ğŸ“‘ ã‚¹ãƒ†ãƒƒãƒ—â‘¢ è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ã‚½ãƒ¼ãƒˆã¨æ•´å½¢ =====")
    sort_yahoo_sheet(gc)
    
    # â‘£ ã‚¹ãƒ†ãƒƒãƒ—â‘£ Geminiåˆ†æã®å®Ÿè¡Œã¨å³æ™‚åæ˜  (G, H, Iåˆ—)
    analyze_with_gemini_and_update_sheet(gc)
    
    print("\n--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œäº† ---")

if __name__ == '__main__':
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ã—ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã‚‹ã‚ˆã†ã«ã™ã‚‹
    if os.path.dirname(os.path.abspath(__file__)) not in sys.path:
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        
    main()
