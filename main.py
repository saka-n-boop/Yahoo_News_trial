import gspread
import time
import re
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
import requests
from urllib.parse import urlparse, parse_qs, urlunparse, urlencode

# --- è¨­å®šï¼ˆå¤‰æ›´ã—ãªã„ã“ã¨ï¼‰ ---
MAX_PAGES = 10 
MAX_RETRIES = 3
JST_HOUR_OFFSET = 9
MAX_SHEET_ROWS_FOR_REPLACE = 10000 
TZ_JST = timezone(timedelta(hours=JST_HOUR_OFFSET), 'JST')

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã¨ã‚·ãƒ¼ãƒˆå (ä½¿ç”¨ã™ã‚‹ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„)
SOURCE_SPREADSHEET_ID = 'ã€ã“ã“ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®IDã‚’è²¼ã‚Šä»˜ã‘ã€‘'
SOURCE_SHEET_NAME = 'SOURCE'
YAHOO_SHEET_HEADERS = ["URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥æ™‚", "æƒ…å ±å…ƒ", "æœ¬æ–‡", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°"]
# -----------------------------

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---

def jst_now():
    """ç¾åœ¨ã®JSTæ™‚åˆ»ã‚’å–å¾—ã™ã‚‹"""
    return datetime.now(TZ_JST)

def format_datetime(dt):
    """datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ 'YY/MM/DD HH:MM' å½¢å¼ã®æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹"""
    if dt:
        return dt.strftime('%y/%m/%d %H:%M')
    return ""

def parse_post_date(date_str, base_time):
    """
    æ—¥ä»˜æ–‡å­—åˆ—ã‚’è§£æã—ã€datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    æ—¥æ™‚æƒ…å ±ãŒãªã„å ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‚’è¿”ã™ã€‚
    """
    if not date_str or date_str == "å–å¾—ä¸å¯":
        return None

    # æœ¬æ–‡å†…æ—¥ä»˜è£œå®Œã§æ®‹ã£ãŸæ›œæ—¥è¡¨è¨˜ã‚’å‰Šé™¤ (ä¾‹: 10/20(æœˆ) 15:30 -> 10/20 15:30)
    s = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", date_str.strip())
    
    # å¹´ã‚’è£œå®Œã™ã‚‹ï¼ˆYY/MM/DDå½¢å¼ã«ã™ã‚‹ï¼‰
    current_year_str = str(base_time.year)[2:]
    
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ï¼ˆJSTï¼‰ã‚’æŒã¤datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦å‡¦ç†
    try:
        if 'é…ä¿¡' in s:
            # ä¾‹: 10/20 15:30é…ä¿¡
            s = s.replace('é…ä¿¡', '').strip()
            dt_obj = datetime.strptime(f"{current_year_str}/{s}", '%y/%m/%d %H:%M')
        elif len(s.split('/')) == 2:
             # ä¾‹: 10/20 (æ—¥æ™‚æƒ…å ±ãªã—ã€æ—¥ä»˜ã®ã¿ã®å ´åˆ)
            dt_obj = datetime.strptime(f"{current_year_str}/{s}", '%y/%m/%d')
        elif len(s.split('/')) == 3:
            # ä¾‹: 25/10/20 (ã‚½ãƒ¼ãƒˆå¾Œã®å½¢å¼ã®å ´åˆ)
            dt_obj = datetime.strptime(s, '%y/%m/%d %H:%M')
        else:
            return None # è§£æå¤±æ•—

        # å¹´ãŒæœªæ¥ï¼ˆç¾åœ¨æœˆã®ç¿Œæœˆä»¥é™ï¼‰ã§ã‚ã‚Œã°ã€å‰å¹´ã«ä¿®æ­£ã™ã‚‹
        if dt_obj.replace(tzinfo=TZ_JST) > base_time + timedelta(days=31):
            dt_obj = dt_obj.replace(year=dt_obj.year - 1)

        return dt_obj.replace(tzinfo=TZ_JST)
        
    except ValueError:
        return None


def request_with_retry(url, retries=MAX_RETRIES):
    """ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    for i in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status() # 4xx, 5xxã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹
            response.encoding = response.apparent_encoding # æ–‡å­—åŒ–ã‘å¯¾ç­–
            return response
        except requests.exceptions.RequestException as e:
            print(f"  â†ªï¸ ãƒªãƒˆãƒ©ã‚¤ {i + 1}/{retries}: {url} - {e}")
            if i < retries - 1:
                time.sleep(2 ** i)
            else:
                return None
    return None

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•° ---

def fetch_article_body_and_comments(base_url):
    """
    è¨˜äº‹URLã‹ã‚‰æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã™ã‚‹ã€‚
    è¤‡æ•°ãƒšãƒ¼ã‚¸ã«åˆ†ã‹ã‚ŒãŸè¨˜äº‹ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å·¡å›ã™ã‚‹ã€‚
    """
    
    full_body_parts = []
    comment_count = ""
    post_date_from_body = "" # æœ¬æ–‡ã‹ã‚‰æŠ½å‡ºã—ãŸæ—¥æ™‚ (Cåˆ—è£œå®Œç”¨)
    
    # URLã‹ã‚‰è¨˜äº‹IDã‚’å–å¾— (ä¾‹: aaa7c40ed1706ff109ad5e48ccebbfe598805ffd)
    article_id_match = re.search(r'/articles/([a-f0-9]+)', base_url)
    if not article_id_match:
        print("  âš ï¸ è¨˜äº‹IDã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return "", "", "" 
        
    article_id = article_id_match.group(1)
    page_counter = 1
    
    # MAX_PAGESã¾ã§ãƒšãƒ¼ã‚¸ã‚’å·¡å›ï¼ˆãƒšãƒ¼ã‚¸ç•ªå·ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ–¹å¼ï¼‰
    while page_counter <= MAX_PAGES:
        # 1. å·¡å›ç”¨URLã‚’ç”Ÿæˆ
        current_url = f"https://news.yahoo.co.jp/articles/{article_id}?page={page_counter}"

        # 2. HTMLå–å¾—ã¨BeautifulSoupã®åˆæœŸåŒ–
        response = request_with_retry(current_url)
        if not response:
            break
        soup = BeautifulSoup(response.text, 'html.parser')

        # 3. è¨˜äº‹æœ¬æ–‡ã®æŠ½å‡º
        # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ç‰¹å®š
        article_content = soup.find('article') or soup.find('div', class_='article_body') or soup.find('div', class_=re.compile(r'article_detail|article_body'))

        current_body = []
        if article_content:
            # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠå†…ã®å…¨ã¦ã®<p>ã‚¿ã‚°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            # æœ€æ–°ã®HTMLæ§‹é€ ã«å¯¾å¿œã—ãŸã‚»ãƒ¬ã‚¯ã‚¿ (sc-54nboa-0 deLyrJ yjSlinkDirectlink highLightSearchTarget)
            paragraphs = article_content.find_all('p', class_=re.compile(r'sc-\w+-0\s+\w+.*highLightSearchTarget'))
            if not paragraphs: # ã‚‚ã—ä¸Šè¨˜ã‚»ãƒ¬ã‚¯ã‚¿ã§å–å¾—ã§ããªã‘ã‚Œã°ã€æ±ç”¨çš„ãª<p>ã‚¿ã‚°ã‚’è©¦ã™
                paragraphs = article_content.find_all('p') 
                
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text:
                    current_body.append(text)

        # 4. åœæ­¢æ¡ä»¶ã®åˆ¤å®š: æœ¬æ–‡ãŒå–å¾—ã§ããªã‘ã‚Œã°çµ‚äº†
        if not current_body:
            # ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„ã‹ã€æœ¬æ–‡ãŒç©ºã§ã‚ã‚‹ãŸã‚ã€å·¡å›ã‚’çµ‚äº†
            if page_counter > 1:
                 # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã§æœ¬æ–‡ãŒãªã‘ã‚Œã°çµ‚äº†
                break
            # 1ãƒšãƒ¼ã‚¸ç›®ã§æœ¬æ–‡ãŒãªã‘ã‚Œã°ã€Œå–å¾—ä¸å¯ã€ã¨ã—ã¦çµ‚äº†
            else:
                return "æœ¬æ–‡å–å¾—ä¸å¯", "", ""
            
        # 5. å–å¾—ã—ãŸæœ¬æ–‡ã‚’å…¨ä½“ã«è¿½åŠ 
        full_body_parts.append("\n".join(current_body))
        
        # 6. ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¨æœ¬æ–‡å†…æ—¥æ™‚æƒ…å ±ã®æŠ½å‡ºï¼ˆ1ãƒšãƒ¼ã‚¸ç›®ã®ã¿å®Ÿæ–½ï¼‰
        if page_counter == 1:
            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°å–å¾—
            comment_button = soup.find('button', attrs={'data-cl-params': re.compile(r'cmtmod')})
            if comment_button:
                match = re.search(r'(\d+)', comment_button.get_text(strip=True).replace(',', ''))
                if match:
                    comment_count = match.group(1)
            
            # æœ¬æ–‡å†…æ—¥ä»˜å–å¾—ï¼ˆCåˆ—è£œå®Œç”¨ï¼‰
            # æ­£è¦è¡¨ç¾: MM/DD(æ›œæ—¥) HH:MMé…ä¿¡ ã‚’æ¢ã™
            date_match = re.search(r'(\d{1,2}/\d{1,2})\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)(\s*)(\d{1,2}:\d{2})', current_body[0])
            if date_match:
                # æ›œæ—¥ã‚’å‰Šé™¤ã—ãŸå½¢å¼ (ä¾‹: 10/20 15:30)
                post_date_from_body = f"{date_match.group(1)} {date_match.group(3)}"

        # 7. æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
        page_counter += 1
        
    # --- ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã®å‡¦ç† ---
    
    # 8. å…¨æœ¬æ–‡ã‚’çµåˆ
    body_text = "\n\n--- ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Š ---\n\n".join(full_body_parts)

    return body_text, comment_count, post_date_from_body


def update_row_details(gc: gspread.Client, row_index, current_row, base_url):
    """
    æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã—ã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®è¡Œã‚’æ›´æ–°ã™ã‚‹ã€‚
    æŠ•ç¨¿æ—¥æ™‚ãŒã€Œå–å¾—ä¸å¯ã€ã®å ´åˆã¯æœ¬æ–‡ã‹ã‚‰è£œå®Œã‚’è©¦ã¿ã‚‹ã€‚
    """
    
    print(f"[{row_index+2}è¡Œç›®] {base_url} ã‹ã‚‰è©³ç´°æƒ…å ±å–å¾—é–‹å§‹...")

    url, title, post_date_raw, source, body, comment_count = current_row
    
    # A-Dåˆ—å–å¾—æ™‚ã«æŠ•ç¨¿æ—¥æ™‚ãŒå–å¾—ä¸å¯/ç©ºæ¬„ã€ã¾ãŸã¯æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãŒç©ºæ¬„ã®å ´åˆã«ã®ã¿å®Ÿè¡Œ
    needs_details = not body.strip() or not comment_count.strip() or "å–å¾—ä¸å¯" in post_date_raw or not post_date_raw.strip()

    if needs_details:
        print("  âœ… æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€ã¾ãŸã¯æ—¥æ™‚ã®è£œå®ŒãŒå¿…è¦ã¨åˆ¤æ–­ã€‚")
        
        # æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€æœ¬æ–‡å†…æ—¥ä»˜ã‚’å–å¾—
        new_body, new_comment_count, post_date_from_body = fetch_article_body_and_comments(base_url)

        updated_row = current_row[:] 
        
        # Eåˆ— (æœ¬æ–‡) ã‚’æ›´æ–°
        if new_body and not body.strip():
            updated_row[4] = new_body
            print("  âœ¨ Eåˆ—(æœ¬æ–‡)ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

        # Fåˆ— (ã‚³ãƒ¡ãƒ³ãƒˆæ•°) ã‚’æ›´æ–°
        if new_comment_count and not comment_count.strip():
            updated_row[5] = new_comment_count
            print("  âœ¨ Fåˆ—(ã‚³ãƒ¡ãƒ³ãƒˆæ•°)ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

        # Cåˆ— (æŠ•ç¨¿æ—¥æ™‚) ã‚’è£œå®Œ
        if ("å–å¾—ä¸å¯" in post_date_raw or not post_date_raw.strip()) and post_date_from_body:
            now = jst_now()
            # æœ¬æ–‡å†…æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            dt = parse_post_date(post_date_from_body, now)
            if dt:
                updated_row[2] = format_datetime(dt)
                print(f"  âœ¨ Cåˆ—(æŠ•ç¨¿æ—¥æ™‚)ã‚’æœ¬æ–‡å†…æ—¥ä»˜ ({updated_row[2]}) ã§è£œå®Œã—ã¾ã—ãŸã€‚")
            
        # æ›´æ–°ã•ã‚ŒãŸè¡Œã‚’è¿”å´
        return updated_row
    else:
        print("  â†ªï¸ æ—¢ã«ã™ã¹ã¦ã®è©³ç´°æƒ…å ±ãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return current_row


def fetch_details_and_update_sheet(gc: gspread.Client):
    """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®å„è¡Œã‚’å·¡å›ã—ã€æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ãƒ»æ›´æ–°ã™ã‚‹ã€‚"""
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        worksheet = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print(f"ã‚¨ãƒ©ãƒ¼: ã‚·ãƒ¼ãƒˆå '{SOURCE_SHEET_NAME}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    all_values = worksheet.get_all_values()
    if not all_values or len(all_values) <= 1:
        print("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    header = all_values[0]
    data_rows = all_values[1:]
    updated_data_rows = []
    
    # è¨˜äº‹è©³ç´°ã®å–å¾—ã¨æ›´æ–°
    for i, row in enumerate(data_rows):
        # è¡Œã®é•·ã•ãŒãƒ˜ãƒƒãƒ€ãƒ¼ã®æ•°ã«æº€ãŸãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if len(row) < len(YAHOO_SHEET_HEADERS):
            # ä¸è¶³ã—ã¦ã„ã‚‹ã‚»ãƒ«ã‚’ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹
            row.extend([''] * (len(YAHOO_SHEET_HEADERS) - len(row)))
        
        url = row[0]
        if url.startswith('http'):
            updated_row = update_row_details(gc, i, row, url)
            updated_data_rows.append(updated_row)
        else:
            updated_data_rows.append(row) # URLãŒç„¡åŠ¹ãªè¡Œã¯ãã®ã¾ã¾è¿½åŠ 

    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ä¸€æ‹¬æ›¸ãè¾¼ã¿
    if updated_data_rows:
        data_to_write = [header] + updated_data_rows
        # æ›¸ãè¾¼ã¿ç¯„å›²ã‚’è¨­å®š
        range_end = gspread.utils.rowcol_to_a1(len(data_to_write), len(YAHOO_SHEET_HEADERS))
        range_name = f'A1:{range_end}'
        
        # ä¸€æ‹¬æ›´æ–°
        worksheet.update(values=data_to_write, range_name=range_name, value_input_option='USER_ENTERED')
        print(f"\nğŸŒŸ {len(updated_data_rows)} è¡Œã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")


def sort_yahoo_sheet(gc: gspread.Client):
    """Cåˆ—ã®æ›œæ—¥ã‚’å‰Šé™¤ã—ã€Cåˆ—ï¼ˆæŠ•ç¨¿æ—¥æ™‚ï¼‰ã§æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹"""
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        worksheet = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("ã‚½ãƒ¼ãƒˆã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # æ›œæ—¥å‰Šé™¤ã®å¯¾è±¡ç¯„å›²ã‚’ C2:C10000 ã«å›ºå®š
    target_range = f"C2:C{MAX_SHEET_ROWS_FOR_REPLACE}" 

    # --- ä¿®æ­£: æ›œæ—¥ã”ã¨ã«å€‹åˆ¥ã®findReplaceãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç”Ÿæˆ (batch_updateæ–¹å¼) ---
    try:
        requests = []
        
        # æ›œæ—¥ãƒªã‚¹ãƒˆ
        days_of_week = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
        
        # 1. å„æ›œæ—¥ã«å¯¾å¿œã™ã‚‹å€‹åˆ¥ã®ç½®æ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç”Ÿæˆ
        for day in days_of_week:
            requests.append({
                "findReplace": {
                    "range": target_range, 
                    "find": rf"\({day}\)", 
                    "replacement": "", 
                    "searchByRegex": True,
                }
            })
            
        # 2. æ›œæ—¥ã®ç›´å¾Œã«æ®‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã€åŠè§’ã‚¹ãƒšãƒ¼ã‚¹1ã¤ã«çµ±ä¸€
        requests.append({
            "findReplace": {
                "range": target_range,
                "find": r"\s{2,}", 
                "replacement": " ", 
                "searchByRegex": True,
            }
        })
        
        # batch_update ã§ã¾ã¨ã‚ã¦å®Ÿè¡Œ
        worksheet.spreadsheet.batch_update({"requests": requests})
        print(" âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§Cåˆ—ã®**æ›œæ—¥è¨˜è¼‰ã‚’å€‹åˆ¥ã«å‰Šé™¤**ã—ã¾ã—ãŸã€‚")
        
    except Exception as e:
        print(f" âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã®ç½®æ›ã‚¨ãƒ©ãƒ¼: {e}") 
    # ----------------------------------------------------


    all_values = worksheet.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        return
        
    header = all_values[0]
    rows = all_values[1:]
    
    now = jst_now()
    def sort_key(row):
        # æŠ•ç¨¿æ—¥æ™‚ï¼ˆCåˆ—ï¼‰ã«åŸºã¥ã„ã¦ã‚½ãƒ¼ãƒˆã‚­ãƒ¼ã‚’æ±ºå®š
        if len(row) > 2:
            dt = parse_post_date(str(row[2]), now)
            # è§£æå¤±æ•—æ™‚ã¯æœ€ã‚‚å¤ã„æ™‚åˆ»ã‚’è¿”ã™ã“ã¨ã§ãƒªã‚¹ãƒˆã®æœ€å¾Œã«ãªã‚‹ã‚ˆã†ã«ã™ã‚‹
            return dt if dt else datetime.min.replace(tzinfo=TZ_JST) 
        else:
            return datetime.min.replace(tzinfo=TZ_JST)
        
    # æ–°ã—ã„é † (reverse=True) ã«ã‚½ãƒ¼ãƒˆ
    sorted_rows = sorted(rows, key=sort_key, reverse=True) 
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®è¡Œã‚’çµåˆã—ã¦æ›¸ãè¾¼ã¿
    full_data_to_write = [header] + sorted_rows
    range_end = gspread.utils.rowcol_to_a1(len(full_data_to_write), len(YAHOO_SHEET_HEADERS))
    
    worksheet.update(values=full_data_to_write, range_name=f'A1:{range_end}', value_input_option='USER_ENTERED')
    
    print(" âœ… SOURCEã‚·ãƒ¼ãƒˆã‚’æŠ•ç¨¿æ—¥æ™‚ã®**æ–°ã—ã„é †**ã«ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---
def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹"""
    print("===== ğŸ”§ ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹å§‹ =====")
    try:
        # èªè¨¼æƒ…å ±ï¼ˆcredentials.jsonï¼‰ã‚’æº–å‚™
        gc = gspread.service_account(filename='credentials.json')

        # ã‚¹ãƒ†ãƒƒãƒ—â‘ : è¨˜äº‹è©³ç´°ã®å–å¾—ã¨æ›´æ–°
        print("\n===== ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—â‘  è¨˜äº‹è©³ç´°ã®å–å¾—ã¨æ›´æ–° =====")
        fetch_details_and_update_sheet(gc)

        # ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ã‚½ãƒ¼ãƒˆå‰ã®Cåˆ—æ•´å½¢
        # ã“ã®å‡¦ç†ã¯ sort_yahoo_sheet é–¢æ•°ã«çµ±åˆã•ã‚Œã€ã‚½ãƒ¼ãƒˆå‰ã«è‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
        
        # ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å…¨ä»¶ã‚½ãƒ¼ãƒˆå®Ÿè¡Œ
        print("\n===== ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—â‘¡ å…¨ä»¶ã‚½ãƒ¼ãƒˆå®Ÿè¡Œ =====")
        sort_yahoo_sheet(gc)

    except gspread.exceptions.APIError as e:
        print(f"\nè‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: Google Sheets APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚èªè¨¼æƒ…å ±ã€IDã€ã‚·ãƒ¼ãƒˆåã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
    except FileNotFoundError:
        print("\nè‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: 'credentials.json' ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    except Exception as e:
        print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    print("\n===== ğŸ”§ ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº† =====")

if __name__ == "__main__":
    main()
