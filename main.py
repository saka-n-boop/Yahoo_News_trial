# -*- coding: utf-8 -*-
"""
çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒˆãƒ¨ã‚¿ç‰ˆï¼‰ - æœ€çµ‚è¨­å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼š
1. Yahooã‚·ãƒ¼ãƒˆã«è¨˜äº‹ãƒªã‚¹ãƒˆã‚’è¿½è¨˜ã—ã€æŠ•ç¨¿æ—¥ã®å¤ã„é †ã«ä¸¦ã³æ›¿ãˆ (A-Dåˆ—)ã€‚
2. Yahooã‚·ãƒ¼ãƒˆã®C-Iåˆ—ã«å¯¾ã—ã€æœ¬æ–‡ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€æ—¥æ™‚è£œå®Œã€Geminiåˆ†æã‚’å®Ÿè¡Œã—ã€ç©ºæ¬„ãŒã‚ã‚Œã°æ›´æ–°ã€‚
"""

import os
import json
import time
import re
import random
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional, Set, Dict, Any

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
# WebDriverWait é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå®‰å®šæ€§å‘ä¸Šã®ãŸã‚ï¼‰
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# --- Gemini API é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from google import genai
from google.genai import types
from google.api_core.exceptions import ResourceExhausted 
# ------------------------------------

# ====== è¨­å®š ======
# â˜…æŒ‡å®šã•ã‚ŒãŸã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’ä½¿ç”¨
SHARED_SPREADSHEET_ID = "1Ru2DT_zzKjTJptchWJitCb67VoffImGhgeOVjwlKukc"
KEYWORD = "ãƒˆãƒ¨ã‚¿"
SOURCE_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
SOURCE_SHEET_NAME = "Yahoo"
DEST_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
# æ–°ã—ã„ãƒ˜ãƒƒãƒ€ãƒ¼æ§‹æˆ
YAHOO_SHEET_HEADERS = ["URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥æ™‚", "ã‚½ãƒ¼ã‚¹", "æœ¬æ–‡", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ãƒã‚¸ãƒã‚¬åˆ†é¡", "ã‚«ãƒ†ã‚´ãƒªåˆ†é¡", "é–¢é€£åº¦"]
MAX_BODY_PAGES = 1
REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}
TZ_JST = timezone(timedelta(hours=9))

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®æŒ‡å®š
PROMPT_FILES = [
    "prompt_gemini_role.txt",
    "prompt_posinega.txt",
    "prompt_category.txt",
    "prompt_score.txt"
]

# --- Gemini ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ– ---
try:
    GEMINI_CLIENT = genai.Client()
except Exception as e:
    print(f"è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    GEMINI_CLIENT = None
# ------------------------------------

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä¿æŒ
GEMINI_PROMPT_TEMPLATE = None

# ====== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ ======

def jst_now() -> datetime:
    return datetime.now(TZ_JST)

def format_datetime(dt_obj) -> str:
    return dt_obj.strftime("%y/%m/%d %H:%M")

def parse_post_date(raw, today_jst: datetime) -> Optional[datetime]:
    """ rawã®æ—¥æ™‚æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã€‚å¹´ãŒãªã„å ´åˆã¯ä»Šå¹´ã‚’è£œå®Œã€‚ """
    if raw is None: return None
    if isinstance(raw, str):
        s = raw.strip()
        s = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", s).strip()
        s = s.strip()
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã®æŠ½å‡ºå½¢å¼: "MM/DD HH:MM"
        for fmt in ("%y/%m/%d %H:%M", "%m/%d %H:%M", "%Y/%m/%d %H:%M", "%Y/%m/%d %H:%M:%S"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m/%d %H:%M":
                    # å¹´ãŒãªã„å ´åˆã¯ä»Šå¹´ã®å¹´ã‚’è£œå®Œã™ã‚‹
                    dt = dt.replace(year=today_jst.year)
                return dt.replace(tzinfo=TZ_JST)
            except ValueError:
                pass
        return None

def build_gspread_client() -> gspread.Client:
    """ GCP_SERVICE_ACCOUNT_KEYç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨ã—ã¦èªè¨¼ """
    try:
        creds_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
            return gspread.authorize(credentials)
        else:
            raise RuntimeError("Googleèªè¨¼æƒ…å ± (GCP_SERVICE_ACCOUNT_KEY) ãŒç’°å¢ƒå¤‰æ•°ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")

def load_gemini_prompt() -> str:
    """ 4ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’çµåˆã—ã¦è¿”ã™ """
    global GEMINI_PROMPT_TEMPLATE
    if GEMINI_PROMPT_TEMPLATE is not None:
        return GEMINI_PROMPT_TEMPLATE
        
    combined_instructions = []
    
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # 1. ãƒ­ãƒ¼ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (prompt_gemini_role.txt) ã‚’æœ€åˆã«èª­ã¿è¾¼ã‚€
        role_file = PROMPT_FILES[0]
        file_path = os.path.join(script_dir, role_file)
        with open(file_path, 'r', encoding='utf-8') as f:
            role_instruction = f.read().strip()
        
        # 2. æ®‹ã‚Šã®åˆ†é¡ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        for filename in PROMPT_FILES[1:]:
            file_path = os.path.join(script_dir, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    combined_instructions.append(content)
                
        if not role_instruction or not combined_instructions:
            print("è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒä¸å®Œå…¨ã¾ãŸã¯ç©ºã§ã™ã€‚")
            return ""

        # å…¨ä½“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        base_prompt = role_instruction + "\n" + "\n".join(combined_instructions)
        
        # è¨˜äº‹æœ¬æ–‡ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’è¿½åŠ 
        base_prompt += "\n\nè¨˜äº‹æœ¬æ–‡:\n{TEXT_TO_ANALYZE}"

        GEMINI_PROMPT_TEMPLATE = base_prompt
        print(f" Geminiãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ {PROMPT_FILES} ã‹ã‚‰èª­ã¿è¾¼ã¿ã€çµåˆã—ã¾ã—ãŸã€‚")
        return base_prompt
        
    except FileNotFoundError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å: {e.filename}")
        return ""
    except Exception as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return ""

# ====== Gemini åˆ†æé–¢æ•° ======

def analyze_with_gemini(text_to_analyze: str) -> Tuple[str, str, str]:
    if not GEMINI_CLIENT:
        return "N/A", "N/A", "0"
        
    if not text_to_analyze.strip():
        return "N/A", "N/A", "0"

    prompt_template = load_gemini_prompt()
    if not prompt_template:
        return "ERROR(Prompt Missing)", "ERROR", "0"

    MAX_RETRIES = 3 
    # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€: æœ€å¤§æ–‡å­—æ•°ã‚’15000ã«è¨­å®š â˜…â˜…â˜…
    MAX_CHARACTERS = 15000 
    # ------------------------------------------

    for attempt in range(MAX_RETRIES):
        try:
            # è¨˜äº‹æœ¬æ–‡ã‚’æŒ‡å®šæ–‡å­—æ•°ã«åˆ¶é™
            text_for_prompt = text_to_analyze[:MAX_CHARACTERS]
            
            prompt = prompt_template.replace("{KEYWORD}", KEYWORD)
            prompt = prompt.replace("{TEXT_TO_ANALYZE}", text_for_prompt)
            
            response = GEMINI_CLIENT.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema={"type": "object", "properties": {
                        "sentiment": {"type": "string", "description": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã®ã„ãšã‚Œã‹"}, 
                        "category": {"type": "string", "description": "ä¼æ¥­ã€ãƒ¢ãƒ‡ãƒ«ã€æŠ€è¡“ãªã©ã®åˆ†é¡çµæœ"}, 
                        "relevance": {"type": "integer", "description": f"{KEYWORD}ã¨ã®é–¢é€£åº¦ã‚’0ã‹ã‚‰100ã®æ•´æ•°"}
                    }}
                ),
            )

            analysis = json.loads(response.text.strip())
            
            sentiment = analysis.get("sentiment", "N/A")
            category = analysis.get("category", "N/A")
            relevance = str(analysis.get("relevance", "0"))

            return sentiment, category, relevance

        except ResourceExhausted as e:
            # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ãƒªãƒˆãƒ©ã‚¤ã›ãšã«çµ‚äº†
            print(f"  ğŸš¨ Gemini API ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
            return "ERROR(Quota)", "ERROR", "0"

        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                wait_time = 2 ** attempt + random.random()
                print(f"  âš ï¸ Gemini API ä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼ã€‚{wait_time:.2f} ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})ã€‚")
                time.sleep(wait_time)
                continue
            else:
                print(f"Geminiåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                return "ERROR", "ERROR", "0"
    return "ERROR", "ERROR", "0"

# ====== ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° ======

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    # A-Dåˆ—ï¼ˆURL, ã‚¿ã‚¤ãƒˆãƒ«, æŠ•ç¨¿æ—¥æ™‚, ã‚½ãƒ¼ã‚¹ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    print("  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹...")
    options = Options()
    options.add_argument("--headless=new") 
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1280,1024")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={REQ_HEADERS['User-Agent']}")
    
    try:
        driver_path = ChromeDriverManager().install()
        service = Service(driver_path)
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f" WebDriverã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []
        
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(search_url)
    
    # è¨˜äº‹ãƒªã‚¹ãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§æœ€å¤§10ç§’å¾…æ©Ÿã™ã‚‹å‡¦ç†ã‚’è¿½åŠ  (å®‰å®šæ€§å‘ä¸Š)
    try:
        # æœ€æ–°ã®ã‚¯ãƒ©ã‚¹å 'sc-1u4589e-0' ã‚’å«ã‚€è¦ç´ ãŒå‡ºç¾ã™ã‚‹ã¾ã§å¾…æ©Ÿ
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "li[class*='sc-1u4589e-0']"))
        )
        print("  è¨˜äº‹ãƒªã‚¹ãƒˆè¦ç´ ã®èª­ã¿è¾¼ã¿ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚")
    except Exception:
        print("  è­¦å‘Š: è¨˜äº‹ãƒªã‚¹ãƒˆè¦ç´ ã®è¡¨ç¤ºã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã¾ã™ã€‚5ç§’å¾…æ©Ÿã—ã¾ã™ã€‚")
        time.sleep(5) # å¾…æ©Ÿå¤±æ•—ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()
    
    # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’æœ€æ–°ã®æ§‹é€ ã«çµ±ä¸€
    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    
    articles_data = []
    
    for article in articles:
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«ã¯ div.sc-3ls169-0 ã®å­è¦ç´ ã¨ã—ã¦å–å¾—
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            
            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag and link_tag["href"].startswith("https://news.yahoo.co.jp/articles/") else ""
            
            # æŠ•ç¨¿æ—¥æ™‚ã®å–å¾—ãƒ­ã‚¸ãƒƒã‚¯
            time_tag = article.find("time")
            # timeã‚¿ã‚°ãŒç›´æ¥è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã€ã‚½ãƒ¼ã‚¹/æ—¥ä»˜ã®ã‚³ãƒ³ãƒ†ãƒŠã‚¯ãƒ©ã‚¹ã‚’æ¤œç´¢ã—ã¦å†æ¤œç´¢
            if not time_tag:
                source_container = article.find("div", class_=re.compile("sc-n3vj8g-0"))
                if source_container:
                    time_tag = source_container.find("time") 
            
            date_str = time_tag.text.strip() if time_tag else ""
            
            # ã‚½ãƒ¼ã‚¹ã®å–å¾—ãƒ­ã‚¸ãƒƒã‚¯
            source_text = ""
            # ã‚¯ãƒ©ã‚¹å 'sc-n3vj8g-0' ã¨ 'sc-110wjhy-8' ã‚’ä½¿ç”¨ã—ã¦å–å¾—
            source_container = article.find("div", class_=re.compile("sc-n3vj8g-0"))
            if source_container:
                inner = source_container.find("div", class_=re.compile("sc-110wjhy-8"))
                if inner and inner.span:
                    # inner.spanãŒæœ€åˆã®è¦ç´ ï¼ˆã‚½ãƒ¼ã‚¹åï¼‰ã§ã‚ã‚‹ã“ã¨ã‚’åˆ©ç”¨
                    candidate_span = inner.find('span') 
                    if candidate_span:
                        candidate = candidate_span.text.strip()
                        # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚¢ã‚¤ã‚³ãƒ³(æ•°å­—)ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                        if not candidate.isdigit():
                            source_text = candidate

            if title and url:
                formatted_date = ""
                if date_str:
                    try:
                        date_str_clean = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", date_str).strip()
                        dt_obj = parse_post_date(date_str_clean, jst_now())
                        if dt_obj:
                            formatted_date = format_datetime(dt_obj)
                    except:
                            formatted_date = date_str

                articles_data.append({
                    "URL": url,
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    # â˜… å–å¾—ä¸å¯ã®å ´åˆã¯ã€Œå–å¾—ä¸å¯ã€ã‚’å…¥ã‚Œã¦ãŠãã€å¾Œã§æœ¬æ–‡ã‹ã‚‰è£œå®Œã™ã‚‹
                    "æŠ•ç¨¿æ—¥æ™‚": formatted_date if formatted_date else "å–å¾—ä¸å¯", 
                    "ã‚½ãƒ¼ã‚¹": source_text
                })
        except Exception as e:
            continue
            
    print(f" Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(articles_data)} ä»¶å–å¾—")
    return articles_data

def fetch_article_body_and_comments(base_url: str) -> Tuple[str, int, Optional[str]]:
    """ è¨˜äº‹æœ¬æ–‡ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€ãŠã‚ˆã³è¨˜äº‹æœ¬æ–‡ã‹ã‚‰æŠ½å‡ºã—ãŸæ—¥æ™‚ã‚’è¿”ã™ """
    body_text = ""
    comment_count = 0
    extracted_date_str = None # â˜… æ–°ã—ãè¿½åŠ 

    try:
        res = requests.get(base_url, headers=REQ_HEADERS, timeout=20)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        
        # è¨˜äº‹æœ¬æ–‡ã®å–å¾—
        article = soup.find("article")
        if article:
            ps = article.find_all("p")
            # æœ€åˆã®æ•°æ®µè½ã‚’çµåˆã—ã¦æ—¥æ™‚æŠ½å‡ºã«ä½¿ã†
            body_text_partial = " ".join(p.get_text(strip=True) for p in ps[:3] if p.get_text(strip=True))
            body_text = "\n".join(p.get_text(strip=True) for p in ps if p.get_text(strip=True))
            
            # â˜… ä¿®æ­£ç‚¹1: è¨˜äº‹æœ¬æ–‡ã®å†’é ­ã‹ã‚‰é…ä¿¡æ—¥æ™‚ã‚’æŠ½å‡º â˜…
            # ä¾‹: 10/15(æ°´)19:10é…ä¿¡ ã¾ãŸã¯ 10/15(æ°´) 19:10é…ä¿¡
            match = re.search(r'(\d{1,2}/\d{1,2})\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)(\s*)(\d{1,2}:\d{2})é…ä¿¡', body_text_partial)
            if match:
                month_day = match.group(1)
                time_str = match.group(3)
                
                # 'MM/DD HH:MM' ã®å½¢å¼ã«å¤‰æ›
                extracted_date_str = f"{month_day} {time_str}"
            # ----------------------------------------------------
        
        # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—
        comment_button = soup.find("button", attrs={"data-cl-params": re.compile(r"cmtmod")})
        
        if comment_button:
            hidden_div = comment_button.find("div", class_="riff-VisuallyHidden__root")
            
            if hidden_div:
                text = hidden_div.get_text(strip=True).replace(",", "")
            else:
                text = comment_button.get_text(strip=True).replace(",", "")
            
            match = re.search(r'(\d+)', text)
            
            if match:
                comment_count = int(match.group(1))

    except Exception as e:
        print(f"    ! è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
    return body_text, comment_count, extracted_date_str # â˜… æˆ»ã‚Šå€¤ã«è¿½åŠ 


# ====== ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ“ä½œé–¢æ•° ======

def ensure_source_sheet_headers(sh: gspread.Spreadsheet) -> gspread.Worksheet:
    """ Yahooã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä¿è¨¼ã™ã‚‹ """
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=SOURCE_SHEET_NAME, rows="3000", cols=str(len(YAHOO_SHEET_HEADERS)))
        
    current_headers = ws.row_values(1)
    if current_headers != YAHOO_SHEET_HEADERS:
        ws.update(range_name=f'A1:{gspread.utils.rowcol_to_a1(1, len(YAHOO_SHEET_HEADERS))}', values=[YAHOO_SHEET_HEADERS])
    return ws

def write_and_sort_news_list_to_source(gc: gspread.Client, articles: list[dict]):
    """ â‘  Yahooã‚·ãƒ¼ãƒˆã«è¨˜äº‹ã‚’è¿½è¨˜ã—ã€â‘¡ å¤ã„é †ã«ä¸¦ã³æ›¿ãˆã‚‹ """
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    worksheet = ensure_source_sheet_headers(sh)
            
    existing_data = worksheet.get_all_values(value_render_option='UNFORMATTED_VALUE')
    existing_urls = set(row[0] for row in existing_data[1:] if len(row) > 0) 
    
    new_data = [[a['URL'], a['ã‚¿ã‚¤ãƒˆãƒ«'], a['æŠ•ç¨¿æ—¥æ™‚'], a['ã‚½ãƒ¼ã‚¹']] for a in articles if a['URL'] not in existing_urls]
    
    if new_data:
        worksheet.append_rows(new_data, value_input_option='USER_ENTERED')
        print(f" SOURCEã‚·ãƒ¼ãƒˆã« {len(new_data)} ä»¶è¿½è¨˜ã—ã¾ã—ãŸã€‚")
        
        all_values = worksheet.get_all_values()
        header = all_values[0]
        rows = all_values[1:]
        
        now = jst_now()
        def sort_key(row):
            # æŠ•ç¨¿æ—¥æ™‚ (Cåˆ—) ã§ã‚½ãƒ¼ãƒˆ
            if len(row) > 2:
                dt = parse_post_date(row[2], now)
                # æ—¥æ™‚ãŒå–å¾—ã§ããªã„å ´åˆã¯ä¸€ç•ªå¤ã„ã‚‚ã®ã¨ã—ã¦æ‰±ã†
                return dt if dt else datetime.max.replace(tzinfo=TZ_JST) 
            else:
                return datetime.max.replace(tzinfo=TZ_JST)
            
        sorted_rows = sorted(rows, key=sort_key) 
        
        full_data_to_write = [header] + sorted_rows
        range_end = gspread.utils.rowcol_to_a1(len(full_data_to_write), len(YAHOO_SHEET_HEADERS))
        
        # ä¸¦ã³æ›¿ãˆãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ¼ãƒˆã«æ›¸ãæˆ»ã™
        worksheet.update(values=full_data_to_write, range_name=f'A1:{range_end}', value_input_option='USER_ENTERED')
        
        print(" SOURCEã‚·ãƒ¼ãƒˆã‚’æŠ•ç¨¿æ—¥æ™‚ã®å¤ã„é †ã«ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")
    else:
        print(" SOURCEã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")


def process_and_update_yahoo_sheet(gc: gspread.Client):
    """ Cï½Iåˆ—ãŒæœªå…¥åŠ›ã®è¡Œã«å¯¾ã—ã€è©³ç´°å–å¾—ã¨Geminiåˆ†æã‚’å®Ÿè¡Œã™ã‚‹ """
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    ws = sh.worksheet(SOURCE_SHEET_NAME)
    
    all_values = ws.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€è©³ç´°å–å¾—ãƒ»åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
        
    data_rows = all_values[1:]
    updates_dict: Dict[int, List[Any]] = {} 
    
    for idx, data_row in enumerate(data_rows):
        row_num = idx + 2 
        
        # A-Dåˆ—ã®å€¤ã‚’å–å¾—
        url = data_row[0] if len(data_row) > 0 else ""
        title = data_row[1] if len(data_row) > 1 else "ä¸æ˜"
        post_date_raw = data_row[2] if len(data_row) > 2 else "" # Cåˆ—ã®å…ƒã®æŠ•ç¨¿æ—¥æ™‚
        source = data_row[3] if len(data_row) > 3 else ""         # Dåˆ—ã®å…ƒã®ã‚½ãƒ¼ã‚¹
        
        # E-Iåˆ—ã®å€¤ã‚’å–å¾—
        body = data_row[4] if len(data_row) > 4 else ""  
        comment_count = data_row[5] if len(data_row) > 5 else ""  
        sentiment = data_row[6] if len(data_row) > 6 else ""
        category = data_row[7] if len(data_row) > 7 else ""
        relevance = data_row[8] if len(data_row) > 8 else ""

        # ãƒ•ãƒ©ã‚°: æœ¬æ–‡ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€ã¾ãŸã¯æ—¥æ™‚ãŒå¿…è¦ã‹ (C, E, Fåˆ—ã®æ›´æ–°ãŒå¿…è¦ãªå ´åˆ)
        needs_details = not body.strip() or not str(comment_count).strip() or "å–å¾—ä¸å¯" in post_date_raw or not post_date_raw.strip()
        
        # ãƒ•ãƒ©ã‚°: Geminiåˆ†æãŒå¿…è¦ã‹ (G, H, Iåˆ—ã®æ›´æ–°ãŒå¿…è¦ãªå ´åˆ)
        needs_analysis = not str(sentiment).strip() or not str(category).strip() or not str(relevance).strip()

        # ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶: ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã‚‹å ´åˆ
        if not needs_details and not needs_analysis:
            continue
            
        if not url.strip():
            print(f"  - è¡Œ {row_num}: URLãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue

        print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")

        # --- è©³ç´°å–å¾— (C, E, Fåˆ—ã®è£œå®Œ) ---
        article_body = body
        final_comment_count = comment_count
        final_post_date = post_date_raw

        if needs_details or not article_body.strip(): # æœ¬æ–‡ãŒç©ºã‹ã€è©³ç´°å–å¾—ãŒå¿…è¦ãªå ´åˆ
            fetched_body, fetched_comment_count, extracted_date = fetch_article_body_and_comments(url) 
            
            # æœ¬æ–‡ã®è£œå®Œ
            if not article_body.strip():
                article_body = fetched_body
            
            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®è£œå®Œ
            if not str(final_comment_count).strip() or str(final_comment_count).strip() == '0':
                final_comment_count = fetched_comment_count
            
            # â˜… æŠ•ç¨¿æ—¥æ™‚ã®è£œå®Œ (Cåˆ—) â˜…
            if ("å–å¾—ä¸å¯" in final_post_date or not final_post_date.strip()) and extracted_date:
                dt_obj = parse_post_date(extracted_date, jst_now())
                if dt_obj:
                    final_post_date = format_datetime(dt_obj)
                else:
                    final_post_date = extracted_date # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤±æ•—ãªã‚‰ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦ãŠã

        
        # --- Geminiåˆ†æã‚’å®Ÿè¡Œ (G, H, Iåˆ—) ---
        final_sentiment = sentiment
        final_category = category
        final_relevance = relevance

        if needs_analysis and article_body.strip(): # æœ¬æ–‡ãŒã‚ã‚Šã€åˆ†æãŒå¿…è¦ãªå ´åˆ
            final_sentiment, final_category, final_relevance = analyze_with_gemini(article_body)
            time.sleep(1 + random.random() * 0.5) # APIè² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿ
        elif needs_analysis and not article_body.strip():
             # æœ¬æ–‡ãŒå–ã‚Œãªã‹ã£ãŸãŒåˆ†æãŒå¿…è¦ãªå ´åˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ¼ã‚¯ï¼‰
             final_sentiment, final_category, final_relevance = "N/A(No Body)", "N/A", "0"

        
        # --- æœ€çµ‚çš„ãªæ›´æ–°ãƒ‡ãƒ¼ã‚¿ ---
        # æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•° (E, Fåˆ—)
        new_body = article_body
        new_comment_count = final_comment_count

        # åˆ†æçµæœ (G, H, Iåˆ—)
        new_sentiment = final_sentiment
        new_category = final_category
        new_relevance = final_relevance


        # æœ€çµ‚çš„ãªæ›´æ–°ãƒ‡ãƒ¼ã‚¿ (Cåˆ—ã‹ã‚‰Iåˆ—ã¾ã§)
        updates_dict[row_num] = [
            final_post_date, # Cåˆ— (æŠ•ç¨¿æ—¥æ™‚)
            source,          # Dåˆ— (ã‚½ãƒ¼ã‚¹) - å¤‰æ›´ãªã—
            new_body,        # Eåˆ— (æœ¬æ–‡)
            new_comment_count, # Fåˆ— (ã‚³ãƒ¡ãƒ³ãƒˆæ•°)
            new_sentiment,   # Gåˆ— (ãƒã‚¸ãƒã‚¬)
            new_category,    # Håˆ— (ã‚«ãƒ†ã‚´ãƒª)
            new_relevance    # Iåˆ— (é–¢é€£åº¦)
        ]


    if updates_dict:
        updates_list = []
        rows_to_update = sorted(updates_dict.keys())
        
        # Cåˆ—ã‹ã‚‰Iåˆ—ã¾ã§ã‚’ä¸€æ‹¬ã§æ›´æ–°
        for r_num in rows_to_update:
            range_name = f'C{r_num}:I{r_num}' 
            updates_list.append({
                'range': range_name,
                'values': [updates_dict[r_num]] 
            })
            
        ws.batch_update(updates_list, value_input_option='USER_ENTERED')
        print(f" Yahooã‚·ãƒ¼ãƒˆã® {len(updates_dict)} è¡Œã®C-Iåˆ—ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
    else:
        print(" Yahooã‚·ãƒ¼ãƒˆã§æ–°ãŸã«å–å¾—ãƒ»åˆ†æã™ã¹ãç©ºæ¬„ã®è¡Œã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


# ====== ãƒ¡ã‚¤ãƒ³å‡¦ç† ======

def main():
    print("--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ ---")
    
    try:
        gc = build_gspread_client()
    except RuntimeError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # 1. Yahooã‚·ãƒ¼ãƒˆã«è¨˜äº‹ãƒªã‚¹ãƒˆã‚’è¿½è¨˜ã—ã€æŠ•ç¨¿æ—¥ã®å¤ã„é †ã«ä¸¦ã³æ›¿ãˆ (A-Dåˆ—ã‚’æ›´æ–°)
    yahoo_news_articles = get_yahoo_news_with_selenium(KEYWORD)
    write_and_sort_news_list_to_source(gc, yahoo_news_articles)
    
    # 2. Yahooã‚·ãƒ¼ãƒˆã®E-Iåˆ—ã«å¯¾ã—ã€æœ¬æ–‡ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€æ—¥æ™‚è£œå®Œã€Geminiåˆ†æã‚’å®Ÿè¡Œã—ã€ç©ºæ¬„ãŒã‚ã‚Œã°æ›´æ–°ã€‚
    process_and_update_yahoo_sheet(gc)
    
    print("\n--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ‚äº† ---")

if __name__ == "__main__":
    main()
