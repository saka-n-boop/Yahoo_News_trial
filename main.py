# -*- coding: utf-8 -*-
"""
çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå›½å†…8ç¤¾å¯¾å¿œç‰ˆï¼‰ - æœ€çµ‚è¨­å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼š
1. keywords.txtã‹ã‚‰å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’èª­ã¿è¾¼ã¿ã€é †æ¬¡Yahooã‚·ãƒ¼ãƒˆã«è¨˜äº‹ãƒªã‚¹ãƒˆã‚’è¿½è¨˜ (A-Dåˆ—)ã€‚
2. æŠ•ç¨¿æ—¥æ™‚ã‹ã‚‰æ›œæ—¥ã‚’ç¢ºå®Ÿã«å‰Šé™¤ã—ã€ã‚¯ãƒªãƒ¼ãƒ³ãªå½¢å¼ã§æ ¼ç´ã€‚
3. æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã—ã€è¡Œã”ã¨ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å³æ™‚åæ˜  (E-Fåˆ—)ã€‚
    -> ã€æ”¹ä¿®æ¸ˆã€‘è¨˜äº‹æœ¬æ–‡ã®å–å¾—ã¯**1å›ã®ã¿**ã«åˆ¶é™ã€‚ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®ã¿æ¯å›æ›´æ–°ã€‚
    -> ã€æ”¹ä¿®æ¸ˆã€‘è¨˜äº‹æœ¬æ–‡ã®å–å¾—ã«ãŠã„ã¦ã€è¤‡æ•°ãƒšãƒ¼ã‚¸å·¡å›ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€**1ãƒšãƒ¼ã‚¸ç›®ã®ã¿**ã‚’å–å¾—ã€‚404ãªã©ã§å–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚
4. å…¨è¨˜äº‹ã‚’æŠ•ç¨¿æ—¥ã®æ–°ã—ã„é †ã«ä¸¦ã³æ›¿ãˆ (A-Dåˆ—ã‚’åŸºæº–ã«ã‚½ãƒ¼ãƒˆ)ã€‚
    -> ã€ä¿®æ­£æ¸ˆã€‘ã‚½ãƒ¼ãƒˆæ™‚ã«**æ—¥æ™‚ãƒ‡ãƒ¼ã‚¿ãŒãªã„è¡ŒãŒå¸¸ã«æœ€ä¸‹éƒ¨ã«ãã‚‹ã‚ˆã†**ã‚½ãƒ¼ãƒˆãƒ­ã‚¸ãƒƒã‚¯ã‚’å¼·åŒ–ã€‚
5. ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè¨˜äº‹ã«å¯¾ã—ã€æ–°ã—ã„ã‚‚ã®ã‹ã‚‰Geminiåˆ†æï¼ˆG, H, Iåˆ—ï¼‰ã‚’å®Ÿè¡Œã€‚
    Geminiåˆ†æã§ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ã€ãã“ã§å‡¦ç†ã‚’ä¸­æ–­ã™ã‚‹ã€‚
"""

import os
import json
import time
import re
import random
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional, Set, Dict, Any
import sys 
from urllib.parse import urlparse, parse_qs, urlunparse, urlencode

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# --- Gemini API é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from google import genai
from google.genai import types
from google.api_core.exceptions import ResourceExhausted 
# ------------------------------------

# ====== è¨­å®š ======
SHARED_SPREADSHEET_ID = "1Ru2DT_zzKjTJptchWJitCb67VoffImGhgeOVjwlKukc" 
KEYWORD_FILE = "keywords.txt" 
SOURCE_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
SOURCE_SHEET_NAME = "Yahoo"
DEST_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
# æ›œæ—¥å‰Šé™¤ã®å¯¾è±¡ã¨ã™ã‚‹æœ€å¤§è¡Œæ•°ã‚’10000ã«è¨­å®š
MAX_SHEET_ROWS_FOR_REPLACE = 10000 
MAX_PAGES = 10 # è¨˜äº‹æœ¬æ–‡å–å¾—ã®æœ€å¤§å·¡å›ãƒšãƒ¼ã‚¸æ•° (â€»ãƒ­ã‚¸ãƒƒã‚¯æ”¹ä¿®ã«ã‚ˆã‚Šç¾åœ¨ã¯1ãƒšãƒ¼ã‚¸ã®ã¿å–å¾—)

YAHOO_SHEET_HEADERS = ["URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥æ™‚", "ã‚½ãƒ¼ã‚¹", "æœ¬æ–‡", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "å¯¾è±¡ä¼æ¥­", "ã‚«ãƒ†ã‚´ãƒªåˆ†é¡", "ãƒã‚¸ãƒã‚¬åˆ†é¡"] 
REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}
TZ_JST = timezone(timedelta(hours=9))

PROMPT_FILES = [
    "prompt_gemini_role.txt",
    "prompt_posinega.txt",
    "prompt_category.txt",
    "prompt_target_company.txt" 
]

try:
    # èªè¨¼æƒ…å ±ã®è¨­å®šã‚’ã‚¹ã‚­ãƒƒãƒ—
    # GEMINI_CLIENT = genai.Client()
    GEMINI_CLIENT = None 
except Exception as e:
    print(f"è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    GEMINI_CLIENT = None

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
if 'GEMINI_API_KEY' in os.environ:
    try:
        GEMINI_CLIENT = genai.Client(api_key=os.environ['GEMINI_API_KEY'])
        print("Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’APIã‚­ãƒ¼ã§åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        GEMINI_CLIENT = None
else:
    print("è­¦å‘Š: ç’°å¢ƒå¤‰æ•° 'GEMINI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")

GEMINI_PROMPT_TEMPLATE = None

# ====== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ ======

def jst_now() -> datetime:
    return datetime.now(TZ_JST)

def format_datetime(dt_obj) -> str:
    # ã€ä¿®æ­£ç‚¹â‘ ã€‘æ—¥æ™‚ã®è¡¨ç¤ºå½¢å¼ã‚’ yyyy/mm/dd hh:mm:ss ã«å¤‰æ›´
    return dt_obj.strftime("%Y/%m/%d %H:%M:%S") # 2025/10/08 10:00:28 ã®å½¢å¼

def parse_post_date(raw, today_jst: datetime) -> Optional[datetime]:
    if raw is None: return None
    if isinstance(raw, str):
        s = raw.strip()
        
        # æ›œæ—¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‰Šé™¤ã™ã‚‹æ­£è¦è¡¨ç¾ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
        s = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", s).strip()
        
        # é…ä¿¡ã¨ã„ã†æ–‡å­—ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤
        s = s.replace('é…ä¿¡', '').strip()
        
        # ä¿®æ­£å¾Œã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å«ã‚ã¦ãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
        for fmt in ("%Y/%m/%d %H:%M:%S", "%y/%m/%d %H:%M", "%m/%d %H:%M", "%Y/%m/%d %H:%M"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m/%d %H:%M":
                    # å¹´ãŒãªã„å½¢å¼ã®å ´åˆã€ä»Šå¹´ã‚’é©ç”¨
                    dt = dt.replace(year=today_jst.year)
                
                # å¹´ãŒæœªæ¥ï¼ˆç¾åœ¨æœˆã®ç¿Œæœˆä»¥é™ï¼‰ã§ã‚ã‚Œã°ã€å‰å¹´ã«ä¿®æ­£ã™ã‚‹ (æœˆæ—¥ã®ã¿ã®å½¢å¼ã‚’è€ƒæ…®)
                if dt.replace(tzinfo=TZ_JST) > today_jst + timedelta(days=31):
                    dt = dt.replace(year=dt.year - 1)
                    
                return dt.replace(tzinfo=TZ_JST)
            except ValueError:
                pass
        return None

def build_gspread_client() -> gspread.Client:
    try:
        # ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
        creds_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
            return gspread.authorize(credentials)
        else:
            # GCP_SERVICE_ACCOUNT_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«èªè¨¼ã‚’è©¦ã¿ã‚‹ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
            try:
                return gspread.service_account(filename='credentials.json')
            except FileNotFoundError:
                raise RuntimeError("Googleèªè¨¼æƒ…å ± (GCP_SERVICE_ACCOUNT_KEY)ãŒç’°å¢ƒå¤‰æ•°ã€ã¾ãŸã¯ 'credentials.json' ãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")

def load_keywords(filename: str) -> List[str]:
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            keywords = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        if not keywords:
            raise ValueError("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã«æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return keywords
    except FileNotFoundError:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []
    except Exception as e:
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def load_gemini_prompt() -> str:
    global GEMINI_PROMPT_TEMPLATE
    if GEMINI_PROMPT_TEMPLATE is not None:
        return GEMINI_PROMPT_TEMPLATE
        
    combined_instructions = []
    
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        role_instruction = ""

        role_file = PROMPT_FILES[0]
        file_path = os.path.join(script_dir, role_file)
        with open(file_path, 'r', encoding='utf-8') as f:
            role_instruction = f.read().strip()
        
        for filename in PROMPT_FILES[1:]:
            file_path = os.path.join(script_dir, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            if content:
                combined_instructions.append(content)
                        
        if not role_instruction or not combined_instructions:
            print("è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒä¸å®Œå…¨ã¾ãŸã¯ç©ºã§ã™ã€‚")
            return ""

        base_prompt = role_instruction + "\n" + "\n".join(combined_instructions)
        base_prompt += "\n\nè¨˜äº‹æœ¬æ–‡:\n{TEXT_TO_ANALYZE}"

        GEMINI_PROMPT_TEMPLATE = base_prompt
        print(f" Geminiãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ {PROMPT_FILES} ã‹ã‚‰èª­ã¿è¾¼ã¿ã€çµåˆã—ã¾ã—ãŸã€‚")
        return base_prompt
        
    except FileNotFoundError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å: {e.filename}")
        return ""
    except Exception as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return ""

def request_with_retry(url: str, max_retries: int = 3) -> Optional[requests.Response]:
    """ è¨˜äº‹æœ¬æ–‡å–å¾—ç”¨ã®ãƒªãƒˆãƒ©ã‚¤ä»˜ããƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒ«ãƒ‘ãƒ¼ """
    for attempt in range(max_retries):
        try:
            res = requests.get(url, headers=REQ_HEADERS, timeout=20)
            
            # ğŸ’¡ æ”¹ä¿®ç‚¹â‘¡: 404 Client Error ã®å ´åˆã€ãƒªãƒˆãƒ©ã‚¤ã›ãš None ã‚’è¿”ã—ã¦å³åº§ã«ã‚¹ã‚­ãƒƒãƒ—
            if res.status_code == 404:
                print(f"  âŒ ãƒšãƒ¼ã‚¸ãªã— (404 Client Error): {url}")
                return None
                
            res.raise_for_status()
            return res
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt + random.random()
                print(f"  âš ï¸ æ¥ç¶šã‚¨ãƒ©ãƒ¼ã€ãƒªãƒˆãƒ©ã‚¤ä¸­... ({attempt + 1}/{max_retries})ã€‚å¾…æ©Ÿ: {wait_time:.2f}ç§’")
                time.sleep(wait_time)
            else:
                print(f"  âŒ æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤å¤±æ•—: {e}")
                return None
    return None

# ====== Gemini åˆ†æé–¢æ•° (å¤‰æ›´ãªã—) ======
def analyze_with_gemini(text_to_analyze: str) -> Tuple[str, str, str, bool]: 
    if not GEMINI_CLIENT:
        return "N/A", "N/A", "N/A", False 
        
    if not text_to_analyze.strip():
        return "N/A", "N/A", "N/A", False

    prompt_template = load_gemini_prompt()
    if not prompt_template:
        return "ERROR(Prompt Missing)", "ERROR", "ERROR", False

    MAX_RETRIES = 3 
    MAX_CHARACTERS = 15000 
    
    for attempt in range(MAX_RETRIES):
        try:
            text_for_prompt = text_to_analyze[:MAX_CHARACTERS]
            prompt = prompt_template.replace("{TEXT_TO_ANALYZE}", text_for_prompt)
            
            response = GEMINI_CLIENT.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
               config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema={"type": "object", "properties": {
                        "company_info": {"type": "string", "description": "è¨˜äº‹ã®ä¸»é¡Œä¼æ¥­åã¨ï¼ˆï¼‰å†…ã«å…±åŒé–‹ç™ºä¼æ¥­åã‚’è¨˜è¼‰ã—ãŸçµæœ"},
                        "category": {"type": "string", "description": "ä¼æ¥­ã€ãƒ¢ãƒ‡ãƒ«ã€æŠ€è¡“ãªã©ã®åˆ†é¡çµæœ"}, 
                        "sentiment": {"type": "string", "description": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã®ã„ãšã‚Œã‹"}
                    }}
                ),
            )

            analysis = json.loads(response.text.strip())
            
            company_info = analysis.get("company_info", "N/A") 
            category = analysis.get("category", "N/A")         
            sentiment = analysis.get("sentiment", "N/A")       

            return company_info, category, sentiment, False

        # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã‚’æœ€å„ªå…ˆã§æ•æ‰ã—ã€å¼·åˆ¶çµ‚äº†
        except ResourceExhausted as e:
            print(f"  ğŸš¨ Gemini API ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
            print("\n===== ğŸ›‘ ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚’æ¤œå‡ºã—ãŸãŸã‚ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’ç›´ã¡ã«ä¸­æ–­ã—ã¾ã™ã€‚ =====")
            sys.stdout.flush() 
            sys.exit(1) # ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’çµ‚äº†

        # ã‚¯ã‚©ãƒ¼ã‚¿ä»¥å¤–ã®ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã®ã¿ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã¨ã™ã‚‹
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                wait_time = 2 ** attempt + random.random()
                print(f"  âš ï¸ Gemini API ä¸€æ™‚çš„ãªæ¥ç¶šã¾ãŸã¯å‡¦ç†ã‚¨ãƒ©ãƒ¼ã€‚{wait_time:.2f} ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})ã€‚")
                time.sleep(wait_time)
                continue
            else:
                print(f"Geminiåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                return "ERROR", "ERROR", "ERROR", False 
        
    return "ERROR", "ERROR", "ERROR", False

# ====== ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° (ã‚½ãƒ¼ã‚¹æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£) ======

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword})...")
    options = Options()
    options.add_argument("--headless=new") 
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080") 
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={REQ_HEADERS['User-Agent']}")
    
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    try:
        driver_path = ChromeDriverManager().install()
        service = Service(driver_path)
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f" WebDriverã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []
        
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(search_url)
    
    try:
        WebDriverWait(driver, 20).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "li[class*='sc-1u4589e-0']"))
        )
        time.sleep(3) 
    except Exception as e:
        print(f"  âš ï¸ ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯è¦ç´ æ¤œç´¢ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        time.sleep(5) 
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()
    
    # è¨˜äº‹ãƒªã‚¹ãƒˆã®è¦ªè¦ç´ ã‚’ç‰¹å®š (ã‚»ãƒ¬ã‚¯ã‚¿ã¯é©å®œèª¿æ•´ãŒå¿…è¦ã«ãªã‚‹å ´åˆãŒã‚ã‚‹)
    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    
    articles_data = []
    today_jst = jst_now()
    
    for article in articles:
        try:
            # A. ã‚¿ã‚¤ãƒˆãƒ«
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            
            # B. URL
            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag and link_tag["href"].startswith("https://news.yahoo.co.jp/articles/") else ""
            
            # C. æŠ•ç¨¿æ—¥æ™‚ (Cåˆ—) æŠ½å‡º
            date_str = ""
            time_tag = article.find("time")
            if time_tag:
                date_str = time_tag.text.strip()
            
            # D. ã‚½ãƒ¼ã‚¹ (Dåˆ—) æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„
            source_text = ""
            source_container = article.find("div", class_=re.compile("sc-n3vj8g-0"))
            
            if source_container:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚„ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å¾Œã«ç¶šãæœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
                time_and_comments = source_container.find("div", class_=re.compile("sc-110wjhy-8"))
                
                if time_and_comments:
                    # divå†…ã®å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€æ—¥ä»˜ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã®è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
                    source_candidates = [
                        span.text.strip() for span in time_and_comments.find_all("span") 
                        if not span.find("svg") # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¤ã‚³ãƒ³ã§ã¯ãªã„
                        and not re.match(r'\d{1,2}/\d{1,2}\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)\d{1,2}:\d{2}', span.text.strip()) # æ—¥ä»˜ã§ã¯ãªã„
                    ]
                    # æœ€ã‚‚é•·ã„ï¼ˆã‚½ãƒ¼ã‚¹ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ï¼‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¡ç”¨
                    if source_candidates:
                        source_text = max(source_candidates, key=len)
                        
                    # ä¸Šè¨˜ã§å–å¾—ã§ããªã„å ´åˆã€ç›´ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’æ¢ã™
                    if not source_text:
                        for content in time_and_comments.contents:
                            if content.name is None and content.strip() and not re.match(r'\d{1,2}/\d{1,2}\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)\d{1,2}:\d{2}', content.strip()):
                                source_text = content.strip()
                                break
            
            if title and url:
                formatted_date = ""
                if date_str:
                    try:
                        # å–å¾—ã—ãŸç”Ÿã®æ—¥ä»˜æ–‡å­—åˆ—ã‹ã‚‰æ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                        dt_obj = parse_post_date(date_str, today_jst)
                        
                        if dt_obj:
                            # ä¿®æ­£ã—ãŸ format_datetime ã‚’ä½¿ç”¨ã—ã€yyyy/mm/dd hh:mm:ss å½¢å¼ã§æ ¼ç´
                            formatted_date = format_datetime(dt_obj)
                        else:
                            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯æ›œæ—¥ã ã‘å‰Šé™¤ã—ãŸç”Ÿæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä¿æŒ
                            formatted_date = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", date_str).strip()
                    except:
                        formatted_date = date_str

                articles_data.append({
                    "URL": url,
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "æŠ•ç¨¿æ—¥æ™‚": formatted_date if formatted_date else "å–å¾—ä¸å¯", 
                    "ã‚½ãƒ¼ã‚¹": source_text if source_text else "å–å¾—ä¸å¯"
                })
        except Exception as e:
            continue
            
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(articles_data)} ä»¶å–å¾—")
    return articles_data

# ====== è©³ç´°å–å¾—é–¢æ•° (è¤‡æ•°ãƒšãƒ¼ã‚¸å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€1ãƒšãƒ¼ã‚¸ç›®ã®ã¿å–å¾—ã«ä¿®æ­£) ======
def fetch_article_body_and_comments(base_url: str) -> Tuple[str, int, Optional[str]]:
    """
    è¨˜äº‹IDãƒ™ãƒ¼ã‚¹ã® '?page=N' ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸè¤‡æ•°ãƒšãƒ¼ã‚¸å·¡å›ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€
    1ãƒšãƒ¼ã‚¸ç›®ã®ã¿ã®å–å¾—ã«ä¿®æ­£ã€‚
    """
    comment_count = -1 # ğŸ’¡ æ”¹ä¿®ç‚¹â‘ : ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãŒå–å¾—ã§ããªã„å ´åˆã¯ -1 (æœªå–å¾—)ã¨ã—ã¦ãƒãƒ¼ã‚¯
    extracted_date_str = None
    
    # URLã‹ã‚‰è¨˜äº‹IDã‚’å–å¾— (ä¾‹: aaa7c40ed1706ff109ad5e48ccebbfe598805ffd)
    article_id_match = re.search(r'/articles/([a-f0-9]+)', base_url)
    if not article_id_match:
        print(f"  âŒ URLã‹ã‚‰è¨˜äº‹IDãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {base_url}")
        return "æœ¬æ–‡å–å¾—ä¸å¯", -1, None
        
    # å¸¸ã«1ãƒšãƒ¼ã‚¸ç›®ï¼ˆãƒ™ãƒ¼ã‚¹URLï¼‰ã®ã¿ã‚’å–å¾—
    current_url = base_url.split('?')[0] # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ãƒ™ãƒ¼ã‚¹URLã‚’ç¢ºä¿
    
    # 2. HTMLå–å¾—ã¨BeautifulSoupã®åˆæœŸåŒ–
    response = request_with_retry(current_url) 
    
    if not response:
        # ğŸ’¡ æ”¹ä¿®ç‚¹â‘¡: request_with_retryã§ãƒªãƒˆãƒ©ã‚¤å¾Œã‚‚å–å¾—ã§ããªã‹ã£ãŸå ´åˆï¼ˆ404ã‚’å«ã‚€ï¼‰ã€ã‚¹ã‚­ãƒƒãƒ—
        print(f"  âŒ è¨˜äº‹æœ¬æ–‡ã®å–å¾—ã«å¤±æ•—ã—ãŸãŸã‚ã€æœ¬æ–‡å–å¾—ä¸å¯ã‚’è¿”ã—ã¾ã™ã€‚: {current_url}")
        return "æœ¬æ–‡å–å¾—ä¸å¯", -1, None
        
    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ 1 ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    soup = BeautifulSoup(response.text, 'html.parser')

    # 3. è¨˜äº‹æœ¬æ–‡ã®æŠ½å‡º (ãƒšãƒ¼ã‚¸1ã®ã¿)
    article_content = soup.find('article') or soup.find('div', class_='article_body') or soup.find('div', class_=re.compile(r'article_detail|article_body'))

    current_body = []
    if article_content:
        # æœ€æ–°ã®HTMLæ§‹é€ ã«å¯¾å¿œã—ãŸã‚»ãƒ¬ã‚¯ã‚¿
        paragraphs = article_content.find_all('p', class_=re.compile(r'sc-\w+-0\s+\w+.*highLightSearchTarget'))
        if not paragraphs: # ä¸Šè¨˜ã‚»ãƒ¬ã‚¯ã‚¿ã§å–å¾—ã§ããªã‘ã‚Œã°æ±ç”¨<p>ã‚’è©¦ã™
            paragraphs = article_content.find_all('p')
            
        for p in paragraphs:
            text = p.get_text(strip=True)
            if text:
                current_body.append(text)
    
    # 4. æœ¬æ–‡ã‚’çµåˆ
    body_text = "\n".join(current_body)
    
    # --- ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¨æ—¥æ™‚ ---
    
    # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’è¡¨ã™ãƒœã‚¿ãƒ³ã¾ãŸã¯ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    comment_button = soup.find("button", attrs={"data-cl-params": re.compile(r"cmtmod")}) or \
                        soup.find("a", attrs={"data-cl-params": re.compile(r"cmtmod")})
    if comment_button:
        # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å«ã‚€è¦ç´ ã‹ã‚‰æ•°å­—ã‚’æŠ½å‡º
        text = comment_button.get_text(strip=True).replace(",", "")
        match = re.search(r'(\d+)', text)
        if match:
            comment_count = int(match.group(1)) # 0ä»¥ä¸Šã®å€¤

    # Cåˆ—è£œå®Œç”¨ã®æ—¥æ™‚ã‚’æœ¬æ–‡ã®å†’é ­ã‹ã‚‰æŠ½å‡ºï¼ˆã€Œ10/20(æœˆ) 15:00 é…ä¿¡ã€ãªã©ã®å½¢å¼ï¼‰
    # è¨˜äº‹æœ¬æ–‡ãƒšãƒ¼ã‚¸å†…ã®æ—¥æ™‚è¦ç´ ã‚’æ¢ã™
    time_tag = soup.find('time')
    if time_tag:
        extracted_date_str = time_tag.text.strip()
    
    # 5. çµæœã‚’è¿”ã™
    return body_text if body_text else "æœ¬æ–‡å–å¾—ä¸å¯", comment_count, extracted_date_str

# ====== ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ“ä½œé–¢æ•° (ã‚½ãƒ¼ãƒˆ/ç½®æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£) ======

def clean_and_update_post_dates(worksheet: gspread.Worksheet, all_values: List[List[str]]) -> List[List[str]]:
    """
    Cåˆ—ã®æ—¥ä»˜æ–‡å­—åˆ—ã‹ã‚‰æ›œæ—¥ã‚’å‰Šé™¤ã—ã€ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’ãƒ¡ãƒ¢ãƒªä¸Šã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚
    åŒæ™‚ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ä¸€æ‹¬æ›´æ–°ã‚’è¡Œã„ã¾ã™ã€‚
    """
    header = all_values[0]
    rows = all_values[1:]
    
    update_cells = []
    
    # æœ€åˆã®MAX_SHEET_ROWS_FOR_REPLACEè¡Œã®ã¿ã‚’å‡¦ç†å¯¾è±¡ã¨ã™ã‚‹
    for r_idx, row in enumerate(rows[:MAX_SHEET_ROWS_FOR_REPLACE]):
        if len(row) > 2:
            original_date = str(row[2]).strip()
            cleaned_date = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", original_date).strip()
            
            # å…ƒã®æ–‡å­—åˆ—ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®æ–‡å­—åˆ—ãŒç•°ãªã‚‹ã¨ãã®ã¿æ›´æ–°å¯¾è±¡ã¨ã™ã‚‹
            if original_date != cleaned_date:
                # Cåˆ— (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹2) ã®ã‚»ãƒ«
                cell_row = r_idx + 2 # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ (1) + 1-based index (1)
                cell_col = 3 # Cåˆ—
                update_cells.append(gspread.Cell(cell_row, cell_col, cleaned_date))
                # ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚‚æ›´æ–°
                rows[r_idx][2] = cleaned_date
                
    if update_cells:
        print(f"  ğŸ“ Cåˆ—ã‹ã‚‰æ›œæ—¥ã‚’å‰Šé™¤: {len(update_cells)} ä»¶ã®ã‚»ãƒ«ã‚’ä¸€æ‹¬æ›´æ–°ã—ã¾ã™ã€‚")
        try:
            worksheet.batch_update(update_cells)
        except Exception as e:
            print(f"  âŒ Cåˆ—ã®ä¸€æ‹¬æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            
    # ãƒ¡ãƒ¢ãƒªä¸Šã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸå…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€ï¼‰ã‚’è¿”ã™
    return [header] + rows

def sort_yahoo_sheet(gc: gspread.Client):
    print("\n===== ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—â‘¢ å…¨ä»¶ã‚½ãƒ¼ãƒˆå®Ÿè¡Œ =====")
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        worksheet = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("ã‚½ãƒ¼ãƒˆã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # 1. å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€Cåˆ—ã®æ›œæ—¥ã‚’å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªã¨ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°
    all_values = worksheet.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        print("ã‚½ãƒ¼ãƒˆã‚¹ã‚­ãƒƒãƒ—: ãƒ‡ãƒ¼ã‚¿ãŒãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã§ã™ã€‚")
        return
    
    all_values = clean_and_update_post_dates(worksheet, all_values)
    print(f" ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§Cåˆ—ã®**æ›œæ—¥è¨˜è¼‰ã‚’å€‹åˆ¥ã«å‰Šé™¤ã—ã€ä½“è£ã‚’æ•´ãˆã¾ã—ãŸ**ã€‚")
    
    header = all_values[0]
    rows = all_values[1:]
    
    # 2. ã‚½ãƒ¼ãƒˆå‡¦ç†
    now = jst_now()
    def sort_key(row):
        """Cåˆ—ã®å€¤ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚½ãƒ¼ãƒˆã‚­ãƒ¼ã¨ã—ã¦è¿”ã™ã€‚"""
        if len(row) > 2:
            raw_date_str = str(row[2]).strip()
            
            # ã€é‡è¦ä¿®æ­£ã€‘Cåˆ—ãŒç©ºã€ã¾ãŸã¯ 'å–å¾—ä¸å¯' ã®å ´åˆã¯ã€ç¢ºå®Ÿã«æœ€ã‚‚å¤ã„æ—¥æ™‚ã‚’è¿”ã™
            if not raw_date_str or raw_date_str == "å–å¾—ä¸å¯":
                # datetime.min ã‚’è¿”ã™ã“ã¨ã§ã€æ–°ã—ã„é † (reverse=True) ã®ã‚½ãƒ¼ãƒˆã§æœ«å°¾ã«é€ã‚‰ã‚Œã‚‹
                return datetime.min.replace(tzinfo=TZ_JST)
                
            # ç½®æ›å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ãªå½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
            dt = parse_post_date(raw_date_str, now)
            
            # æ—¥ä»˜ã«å¤‰æ›ã§ããªã„å ´åˆã‚‚ã€æœ€ã‚‚å¤ã„æ—¥æ™‚ã‚’è¿”ã™
            return dt if dt else datetime.min.replace(tzinfo=TZ_JST) 
        else:
            # è¡Œã®é•·ã•ãŒè¶³ã‚Šãªã„å ´åˆã‚‚æœ€ä¸‹éƒ¨ã«é€ã‚‹
            return datetime.min.replace(tzinfo=TZ_JST)
        
    # reverse=True ã«è¨­å®šã—ã€æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹ (ã‚½ãƒ¼ãƒˆã‚­ãƒ¼ã®å€¤ãŒå¤§ãã„ã‚‚ã®(æ–°ã—ã„)ãŒä¸Š)
    sorted_rows = sorted(rows, key=sort_key, reverse=True) 
    
    # 3. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿
    final_data = [header] + sorted_rows
    
    try:
        # A1ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹æœ€çµ‚è¡Œ/æœ€çµ‚åˆ—ã¾ã§ã‚’æ›´æ–°ç¯„å›²ã¨ã™ã‚‹
        # update() ã¯è¡Œã¨åˆ—ã®æ•°ã‚’è‡ªå‹•ã§èª¿æ•´ã—ã€ç©ºç™½ã‚’å‰Šé™¤ã—ã¦ãã‚Œã‚‹
        range_to_update = f'A1:{gspread.utils.rowcol_to_a1(len(final_data), len(header))}'
        worksheet.update(range_to_update, final_data, value_input_option='USER_ENTERED')
        print(f" SOURCEã‚·ãƒ¼ãƒˆã‚’æŠ•ç¨¿æ—¥æ™‚ã®**æ–°ã—ã„é †**ã«ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"  âŒ ã‚½ãƒ¼ãƒˆçµæœã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
    print("========================================")


def append_new_articles_to_sheet(gc: gspread.Client, articles_data: list[dict], keyword: str):
    print(f"\n===== ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—â‘ -2 è¨˜äº‹ãƒªã‚¹ãƒˆã®å·®åˆ†ç¢ºèªã¨è¿½åŠ  ({keyword}) =====")
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        worksheet = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("âŒ ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # 1. æ—¢å­˜ã®URLãƒªã‚¹ãƒˆã‚’å–å¾— (Aåˆ—ã®å…¨ã¦ã®å€¤)
    try:
        existing_urls = set(worksheet.col_values(1))
    except Exception as e:
        print(f"  âŒ æ—¢å­˜URLã®å–å¾—ã«å¤±æ•—: {e}ã€‚å…¨ä»¶ã‚’æ–°è¦ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚")
        existing_urls = set()
        
    new_rows = []
    
    # 2. æ–°è¦è¨˜äº‹ã®æŠ½å‡º
    for article in articles_data:
        url = article['URL']
        if url not in existing_urls:
            # A, B, C, Dåˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦è¿½åŠ 
            # E, F, G, H, Iåˆ—ã¯ç©ºæ¬„ã§åˆæœŸåŒ–
            new_row = [
                article['URL'], 
                article['ã‚¿ã‚¤ãƒˆãƒ«'], 
                article['æŠ•ç¨¿æ—¥æ™‚'], 
                article['ã‚½ãƒ¼ã‚¹'], 
                "", # Eåˆ—: æœ¬æ–‡
                "", # Fåˆ—: ã‚³ãƒ¡ãƒ³ãƒˆæ•°
                "", # Gåˆ—: å¯¾è±¡ä¼æ¥­
                "", # Håˆ—: ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
                ""  # Iåˆ—: ãƒã‚¸ãƒã‚¬åˆ†é¡
            ]
            new_rows.append(new_row)
            existing_urls.add(url) # ã™ã§ã«å‡¦ç†ã—ãŸã¨ãƒãƒ¼ã‚¯

    # 3. æ–°è¦è¨˜äº‹ã®ã‚·ãƒ¼ãƒˆã¸ã®è¿½è¨˜
    if new_rows:
        print(f"  âœ… æ–°è¦è¨˜äº‹ã‚’ {len(new_rows)} ä»¶æ¤œå‡ºã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã™ã€‚")
        try:
            worksheet.append_rows(new_rows, value_input_option='USER_ENTERED')
            print(f"  â¡ï¸ ã‚·ãƒ¼ãƒˆã«è¿½è¨˜å®Œäº†ã€‚")
        except Exception as e:
            print(f"  âŒ è¨˜äº‹ã®è¿½è¨˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    else:
        print("  âœ… æ–°è¦è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    print("========================================")

def update_article_details_and_gemini_analysis(gc: gspread.Client, max_articles: int = 50):
    print("\n===== ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—â‘£ è©³ç´°æƒ…å ±å–å¾—ã¨Geminiåˆ†æå®Ÿè¡Œ =====")
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        worksheet = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("âŒ Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã¨è¨˜äº‹è¡Œã«åˆ†ã‘ã‚‹
    all_values = worksheet.get_all_values()
    if len(all_values) <= 1:
        print("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    header = all_values[0]
    rows = all_values[1:]
    
    update_cells = []
    gemini_analysis_count = 0 

    # ã‚½ãƒ¼ãƒˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ï¼ˆæ–°ã—ã„é †ï¼‰ã®ä¸Šä½max_articlesä»¶ã‚’å‡¦ç†å¯¾è±¡ã¨ã™ã‚‹
    for r_idx, row in enumerate(rows[:max_articles]):
        row_num = r_idx + 2 # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã®è¡Œç•ªå· (ãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€)
        
        # è¨˜äº‹è¡Œã«å¿…è¦ãªåˆ—ãŒæƒã£ã¦ã„ã‚‹ã‹ç¢ºèª (A-Dåˆ—: URL, ã‚¿ã‚¤ãƒˆãƒ«, æŠ•ç¨¿æ—¥æ™‚, ã‚½ãƒ¼ã‚¹)
        if len(row) < 4:
            continue
            
        url = str(row[0]).strip()
        current_body = str(row[4]).strip() if len(row) > 4 else ""
        current_comments = str(row[5]).strip() if len(row) > 5 else ""
        current_sentiment = str(row[8]).strip() if len(row) > 8 else ""
        
        # 1. Eåˆ—(æœ¬æ–‡)ã¨Fåˆ—(ã‚³ãƒ¡ãƒ³ãƒˆæ•°)ã®å–å¾—/æ›´æ–°
        
        new_body = current_body
        new_comments_str = current_comments
        extracted_date_str = None
        
        # Eåˆ—(æœ¬æ–‡)ãŒç©ºã®å ´åˆã®ã¿ã€æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’æ–°è¦å–å¾—
        if not current_body or current_body == "æœ¬æ–‡å–å¾—ä¸å¯":
            print(f"  [Row {row_num}] è¨˜äº‹æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’æ–°è¦å–å¾—ã—ã¾ã™...")
            new_body, comment_count, extracted_date_str = fetch_article_body_and_comments(url)
            
            # Eåˆ—æ›´æ–° (æœ¬æ–‡)
            update_cells.append(gspread.Cell(row_num, 5, new_body)) 
            new_body = new_body # ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚‚æ›´æ–°
            
            # Fåˆ—æ›´æ–° (ã‚³ãƒ¡ãƒ³ãƒˆæ•°)
            if comment_count != -1:
                new_comments_str = str(comment_count)
                update_cells.append(gspread.Cell(row_num, 6, new_comments_str))
                
            # Cåˆ—ã®è£œå®Œ: æœ¬æ–‡å–å¾—æ™‚ã«æ—¥æ™‚ãŒå–ã‚ŒãŸã‚‰Cåˆ—ã‚’æ›´æ–°
            current_date_c = str(row[2]).strip()
            if extracted_date_str and (not current_date_c or current_date_c == "å–å¾—ä¸å¯"):
                # Cåˆ—ã«æ­£ã—ã„å½¢å¼ã§æ—¥æ™‚ã‚’æ ¼ç´ (parse_post_dateé–¢æ•°å†…ã§æ›œæ—¥å‰Šé™¤æ¸ˆã¿)
                today_jst = jst_now()
                dt_obj = parse_post_date(extracted_date_str, today_jst)
                if dt_obj:
                    formatted_date = format_datetime(dt_obj)
                    update_cells.append(gspread.Cell(row_num, 3, formatted_date)) # Cåˆ—
                    rows[r_idx][2] = formatted_date # ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚‚æ›´æ–°
                    
        # Eåˆ—(æœ¬æ–‡)ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®ã¿æ›´æ–°ã‚’è©¦ã¿ã‚‹
        elif current_comments in ("", "æœªå–å¾—", "-1"): 
            # Eåˆ—ã«æœ¬æ–‡ãŒã‚ã‚‹ãŒã€Fåˆ—ã«ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãŒãªã„å ´åˆ
            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®ã¿ã‚’å†å–å¾—ã™ã‚‹ (æœ¬æ–‡ã¯ã‚¹ã‚­ãƒƒãƒ—)
            _, comment_count, _ = fetch_article_body_and_comments(url)
            if comment_count != -1:
                new_comments_str = str(comment_count)
                update_cells.append(gspread.Cell(row_num, 6, new_comments_str))
                
        # 2. G, H, Iåˆ— (Geminiåˆ†æ)ã®å®Ÿè¡Œ
        
        # æœ¬æ–‡ãŒã‚ã‚Šã€ã‹ã¤ãƒã‚¸ãƒã‚¬åˆ†é¡ãŒç©ºæ¬„ã®å ´åˆã®ã¿åˆ†æã‚’å®Ÿè¡Œ
        if new_body and new_body != "æœ¬æ–‡å–å¾—ä¸å¯" and not current_sentiment:
            print(f"  [Row {row_num}] Geminiåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            
            try:
                company, category, sentiment, is_quota_error = analyze_with_gemini(new_body)
                
                if is_quota_error:
                    # analyze_with_geminiå†…ã§å¼·åˆ¶çµ‚äº†ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã¯åˆ°é”ã—ãªã„ã¯ãš
                    # å¿µã®ãŸã‚ã€ã“ã“ã§ä¸­æ–­å‡¦ç†ã‚’è¨­ã‘ã¦ã‚‚è‰¯ã„
                    pass 
                
                # G, H, Iåˆ—ã‚’æ›´æ–°
                update_cells.append(gspread.Cell(row_num, 7, company)) 
                update_cells.append(gspread.Cell(row_num, 8, category)) 
                update_cells.append(gspread.Cell(row_num, 9, sentiment)) 
                gemini_analysis_count += 1
                
                # é€£ç¶šå®Ÿè¡Œã‚’é˜²ããŸã‚ã®å¾…æ©Ÿ
                time.sleep(random.uniform(2, 4)) 
                
            except Exception as e:
                # ã‚¯ã‚©ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ä»¥å¤–ã®ã€äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼
                print(f"  âŒ Geminiåˆ†æã‚¨ãƒ©ãƒ¼ (Row {row_num}): {e}")
                
            if gemini_analysis_count >= 10: # ä¾‹: 10ä»¶åˆ†æã—ãŸã‚‰ä¸€æ™‚ä¸­æ–­
                 print(f"  â³ ä¸€æ™‚ä¸­æ–­: é€£ç¶šã§ {gemini_analysis_count} ä»¶ã®Geminiåˆ†æã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚")
                 break

    # 3. ã‚·ãƒ¼ãƒˆã¸ã®ä¸€æ‹¬æ›´æ–°
    if update_cells:
        print(f"  ğŸ“ è©³ç´°æƒ…å ±ã¨åˆ†æçµæœã‚’ {len(update_cells)} å€‹ã®ã‚»ãƒ«ã§ä¸€æ‹¬æ›´æ–°ã—ã¾ã™ã€‚")
        try:
            worksheet.batch_update(update_cells)
            print("  â¡ï¸ ã‚·ãƒ¼ãƒˆã¸ã®æ›´æ–°å®Œäº†ã€‚")
        except Exception as e:
            print(f"  âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ä¸€æ‹¬æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    else:
        print("  å¤‰æ›´ã®å¿…è¦ã®ã‚ã‚‹ã‚»ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
    print("========================================\n")


# ====== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ ======

def main():
    print("========================================")
    print(" Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹åé›†ãƒ»åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆèµ·å‹•")
    print("========================================")
    
    # 1. èªè¨¼ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿
    try:
        gc = build_gspread_client()
        keywords = load_keywords(KEYWORD_FILE)
        if not keywords:
            print("å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            return
    except RuntimeError as e:
        print(f"è‡´å‘½çš„ãªåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # 2. è¨˜äº‹ã®åé›†ã¨ã‚·ãƒ¼ãƒˆã¸ã®è¿½è¨˜
    all_new_articles = []
    for keyword in keywords:
        print(f"\n===== ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—â‘ -1 è¨˜äº‹ãƒªã‚¹ãƒˆã®åé›† ({keyword}) =====")
        articles_data = get_yahoo_news_with_selenium(keyword)
        
        # ã‚¹ãƒ†ãƒƒãƒ—â‘ -2: åé›†ã—ãŸè¨˜äº‹ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ  (å·®åˆ†ãƒã‚§ãƒƒã‚¯ã‚ã‚Š)
        if articles_data:
            append_new_articles_to_sheet(gc, articles_data, keyword)
        
        # Selenium/WebDriverã®ãƒ—ãƒ­ã‚»ã‚¹ãŒæ®‹ã‚‰ãªã„ã‚ˆã†ã«ç¢ºå®Ÿã«çµ‚äº†
        time.sleep(1) 

    # 3. ã‚½ãƒ¼ãƒˆå®Ÿè¡Œ
    # Cåˆ—ã®ãƒ‘ãƒ¼ã‚¹ä¸è‰¯è¡ŒãŒæœ€ä¸‹éƒ¨ã«é€ã‚‰ã‚Œã‚‹ã‚ˆã†ä¿®æ­£æ¸ˆã¿
    sort_yahoo_sheet(gc)
    
    # 4. è©³ç´°å–å¾—ã¨Geminiåˆ†æå®Ÿè¡Œ
    # ã‚½ãƒ¼ãƒˆå¾Œã®ä¸Šä½50ä»¶ã‚’ç›®å®‰ã«å‡¦ç†
    update_article_details_and_gemini_analysis(gc, max_articles=50) 
    
    print("=========================================")
    print(" âœ… å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    print("=========================================")

if __name__ == "__main__":
    main()
