# -*- coding: utf-8 -*-
"""
çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå›½å†…8ç¤¾å¯¾å¿œç‰ˆï¼‰ - æœ€çµ‚è¨­å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼š
1. keywords.txtã‹ã‚‰å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’èª­ã¿è¾¼ã¿ã€é †æ¬¡Yahooã‚·ãƒ¼ãƒˆã«è¨˜äº‹ãƒªã‚¹ãƒˆã‚’è¿½è¨˜ (A-Dåˆ—)ã€‚
2. æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã—ã€è¡Œã”ã¨ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å³æ™‚åæ˜  (E-Fåˆ—)ã€‚
3. å…¨è¨˜äº‹ã‚’æŠ•ç¨¿æ—¥ã®å¤ã„é †ã«ä¸¦ã³æ›¿ãˆ (A-Dåˆ—ã‚’åŸºæº–ã«ã‚½ãƒ¼ãƒˆ)ã€‚
4. ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè¨˜äº‹ã«å¯¾ã—ã€å¤ã„ã‚‚ã®ã‹ã‚‰Geminiåˆ†æï¼ˆG, H, Iåˆ—ï¼‰ã‚’å®Ÿè¡Œã€‚
   Geminiåˆ†æã§ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ã€ãã“ã§å‡¦ç†ã‚’ä¸­æ–­ã™ã‚‹ã€‚
"""

import os
import json
import time
import re
import random
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional, Set, Dict, Any
import sys 

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# --- Gemini API é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from google import genai
from google.genai import types
from google.api_core.exceptions import ResourceExhausted 
# ------------------------------------

# ====== è¨­å®š (å¤‰æ›´ãªã—) ======
SHARED_SPREADSHEET_ID = "1Ru2DT_zzKjTJptchWJitCb67VoffImGhgeOVyKukc"
KEYWORD_FILE = "keywords.txt" 
SOURCE_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
SOURCE_SHEET_NAME = "Yahoo"
DEST_SPREADSHEET_ID = SHARED_SPREADSHEET_ID

YAHOO_SHEET_HEADERS = ["URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥æ™‚", "ã‚½ãƒ¼ã‚¹", "æœ¬æ–‡", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "å¯¾è±¡ä¼æ¥­", "ã‚«ãƒ†ã‚´ãƒªåˆ†é¡", "ãƒã‚¸ãƒã‚¬åˆ†é¡"] 
REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}
TZ_JST = timezone(timedelta(hours=9))

PROMPT_FILES = [
    "prompt_gemini_role.txt",
    "prompt_posinega.txt",
    "prompt_category.txt",
    "prompt_target_company.txt" 
]

try:
    GEMINI_CLIENT = genai.Client()
except Exception as e:
    print(f"è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    GEMINI_CLIENT = None

GEMINI_PROMPT_TEMPLATE = None

# ====== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ (å¤‰æ›´ãªã—) ======

def jst_now() -> datetime:
    return datetime.now(TZ_JST)

def format_datetime(dt_obj) -> str:
    return dt_obj.strftime("%y/%m/%d %H:%M")

def parse_post_date(raw, today_jst: datetime) -> Optional[datetime]:
    if raw is None: return None
    if isinstance(raw, str):
        s = raw.strip()
        s = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", s).strip()
        s = s.strip()
        for fmt in ("%y/%m/%d %H:%M", "%m/%d %H:%M", "%Y/%m/%d %H:%M", "%Y/%m/%d %H:%M:%S"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m/%d %H:%M":
                    dt = dt.replace(year=today_jst.year)
                return dt.replace(tzinfo=TZ_JST)
            except ValueError:
                pass
        return None

def build_gspread_client() -> gspread.Client:
    try:
        creds_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
            return gspread.authorize(credentials)
        else:
            raise RuntimeError("Googleèªè¨¼æƒ…å ± (GCP_SERVICE_ACCOUNT_KEY) ãŒç’°å¢ƒå¤‰æ•°ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")

def load_keywords(filename: str) -> List[str]:
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            keywords = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        if not keywords:
            raise ValueError("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã«æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return keywords
    except FileNotFoundError:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []
    except Exception as e:
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def load_gemini_prompt() -> str:
    global GEMINI_PROMPT_TEMPLATE
    if GEMINI_PROMPT_TEMPLATE is not None:
        return GEMINI_PROMPT_TEMPLATE
        
    combined_instructions = []
    
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        role_instruction = ""

        role_file = PROMPT_FILES[0]
        file_path = os.path.join(script_dir, role_file)
        with open(file_path, 'r', encoding='utf-8') as f:
            role_instruction = f.read().strip()
        
        for filename in PROMPT_FILES[1:]:
            file_path = os.path.join(script_dir, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    combined_instructions.append(content)
                
        if not role_instruction or not combined_instructions:
            print("è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒä¸å®Œå…¨ã¾ãŸã¯ç©ºã§ã™ã€‚")
            return ""

        base_prompt = role_instruction + "\n" + "\n".join(combined_instructions)
        base_prompt += "\n\nè¨˜äº‹æœ¬æ–‡:\n{TEXT_TO_ANALYZE}"

        GEMINI_PROMPT_TEMPLATE = base_prompt
        print(f" Geminiãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ {PROMPT_FILES} ã‹ã‚‰èª­ã¿è¾¼ã¿ã€çµåˆã—ã¾ã—ãŸã€‚")
        return base_prompt
        
    except FileNotFoundError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å: {e.filename}")
        return ""
    except Exception as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return ""

# ====== Gemini åˆ†æé–¢æ•° (ã‚¯ã‚©ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«å¼·åˆ¶åœæ­¢) ======
def analyze_with_gemini(text_to_analyze: str) -> Tuple[str, str, str, bool]: 
    if not GEMINI_CLIENT:
        return "N/A", "N/A", "N/A", False 
        
    if not text_to_analyze.strip():
        return "N/A", "N/A", "N/A", False

    prompt_template = load_gemini_prompt()
    if not prompt_template:
        return "ERROR(Prompt Missing)", "ERROR", "ERROR", False

    MAX_RETRIES = 3 
    MAX_CHARACTERS = 15000 
    
    for attempt in range(MAX_RETRIES):
        try:
            text_for_prompt = text_to_analyze[:MAX_CHARACTERS]
            prompt = prompt_template.replace("{TEXT_TO_ANALYZE}", text_for_prompt)
            
            response = GEMINI_CLIENT.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema={"type": "object", "properties": {
                        "company_info": {"type": "string", "description": "è¨˜äº‹ã®ä¸»é¡Œä¼æ¥­åã¨ï¼ˆï¼‰å†…ã«å…±åŒé–‹ç™ºä¼æ¥­åã‚’è¨˜è¼‰ã—ãŸçµæœ"},
                        "category": {"type": "string", "description": "ä¼æ¥­ã€ãƒ¢ãƒ‡ãƒ«ã€æŠ€è¡“ãªã©ã®åˆ†é¡çµæœ"}, 
                        "sentiment": {"type": "string", "description": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã®ã„ãšã‚Œã‹"}
                    }}
                ),
            )

            analysis = json.loads(response.text.strip())
            
            company_info = analysis.get("company_info", "N/A") 
            category = analysis.get("category", "N/A")         
            sentiment = analysis.get("sentiment", "N/A")       

            return company_info, category, sentiment, False

        # â˜… ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã‚’æœ€å„ªå…ˆã§æ•æ‰ã—ã€å¼·åˆ¶çµ‚äº†
        except ResourceExhausted as e:
            print(f"  ğŸš¨ Gemini API ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
            print("\n===== ğŸ›‘ ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚’æ¤œå‡ºã—ãŸãŸã‚ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’ç›´ã¡ã«ä¸­æ–­ã—ã¾ã™ã€‚ =====")
            sys.stdout.flush() 
            sys.exit(1) # ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’çµ‚äº†

        # ã‚¯ã‚©ãƒ¼ã‚¿ä»¥å¤–ã®ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã®ã¿ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã¨ã™ã‚‹
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                wait_time = 2 ** attempt + random.random()
                print(f"  âš ï¸ Gemini API ä¸€æ™‚çš„ãªæ¥ç¶šã¾ãŸã¯å‡¦ç†ã‚¨ãƒ©ãƒ¼ã€‚{wait_time:.2f} ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})ã€‚")
                time.sleep(wait_time)
                continue
            else:
                print(f"Geminiåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                return "ERROR", "ERROR", "ERROR", False 
    
    return "ERROR", "ERROR", "ERROR", False

# ====== ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° (æ—¥æ™‚ãƒ»ã‚½ãƒ¼ã‚¹æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£) ======

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword})...")
    options = Options()
    options.add_argument("--headless=new") 
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1280,1024")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={REQ_HEADERS['User-Agent']}")
    
    try:
        driver_path = ChromeDriverManager().install()
        service = Service(driver_path)
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f" WebDriverã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []
        
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(search_url)
    
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "li[class*='sc-1u4589e-0']"))
        )
        time.sleep(1) 
    except Exception:
        time.sleep(5) 
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()
    
    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    
    articles_data = []
    
    for article in articles:
        try:
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            
            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag and link_tag["href"].startswith("https://news.yahoo.co.jp/articles/") else ""
            
            # --- æŠ•ç¨¿æ—¥æ™‚ (Cåˆ—) æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯å†å¼·åŒ– ---
            date_str = ""
            # å„ªå…ˆåº¦1: <time>ã‚¿ã‚°ã‹ã‚‰å–å¾— (æœ€ã‚‚æ­£ç¢º)
            time_tag = article.find("time")
            if time_tag:
                date_str = time_tag.text.strip()
            
            # --- ã‚½ãƒ¼ã‚¹ (Dåˆ—) æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯å†å¼·åŒ– ---
            source_text = ""
            source_container = article.find("div", class_=re.compile("sc-n3vj8g-0"))
            
            if source_container:
                # ã‚½ãƒ¼ã‚¹ã¨æ—¥æ™‚ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆãŒå«ã¾ã‚Œã‚‹ inner div
                inner_div = source_container.find("div", class_=re.compile("sc-110wjhy-8"))

                if inner_div:
                    # æœ€åˆã®SPAMã‚¿ã‚°ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆæ•°ï¼‰ã‚’æ¢ã™
                    comment_span = inner_div.find("span", class_=re.compile("sc-jksony-0"))
                    
                    # æŠ•ç¨¿æ—¥æ™‚ (time) ã¨ã‚«ãƒ†ã‚´ãƒª (span) ã‚’é™¤ãæœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã¾ãŸã¯spanãŒã‚½ãƒ¼ã‚¹
                    
                    # å­è¦ç´ ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                    children = [c for c in inner_div.contents if c.strip() or c.name]
                    
                    # ã‚³ãƒ¡ãƒ³ãƒˆæ•°SPAMã®ç›´å¾Œï¼ˆã¾ãŸã¯inner_divã®æœ€åˆï¼‰ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã¾ãŸã¯SPANã‚’æ¢ã™
                    source_candidate = None
                    if comment_span:
                        # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®æ¬¡ã®è¦ç´ ã‚’æ¤œç´¢
                        next_element = comment_span.find_next_sibling()
                        if next_element and next_element.name == 'span' and not next_element.has_attr('class') or next_element.get('class') == ['sc-110wjhy-9','cnuhon']:
                            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®æ¬¡ã®spanã‚¿ã‚°ã‚’ã‚½ãƒ¼ã‚¹ã¨ã™ã‚‹ï¼ˆä¾‹: â‘ â‘¡â‘¢ã®ãƒ­ã‚¤ã‚¿ãƒ¼ã€MotorFanã€carviewï¼ï¼‰
                            source_candidate = next_element.text.strip()
                        elif next_element and next_element.name == 'span' and next_element.get('class') == ['sc-110wjhy-1', 'hgCcVT']:
                             # ã‚³ãƒ¡ãƒ³ãƒˆã®æ¬¡ã«ã‚«ãƒ†ã‚´ãƒªãŒã‚ã£ãŸå ´åˆã€ãã®æ¬¡ã‚‚ç¢ºèª (ç¨€ã ãŒä¸€å¿œ)
                             source_candidate = comment_span.next_sibling.strip() if comment_span.next_sibling and comment_span.next_sibling.strip() else ""
                        else:
                            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’å–å¾— (ç¨€)
                            source_candidate = comment_span.next_sibling.strip() if comment_span.next_sibling and comment_span.next_sibling.strip() else ""

                    else:
                        # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãŒãªã„å ´åˆã€æœ€åˆã®spanã‚’ã‚½ãƒ¼ã‚¹ã¨ã™ã‚‹ (ã“ã®æ§‹é€ ã¯ç¨€)
                        first_span = inner_div.find("span")
                        if first_span and not first_span.find("svg"):
                             source_candidate = first_span.text.strip()

                    # æœ€çµ‚çš„ãªã‚½ãƒ¼ã‚¹ã®æ±ºå®š
                    if source_candidate:
                        source_text = source_candidate

            # æŠ•ç¨¿æ—¥æ™‚ã¨ã‚½ãƒ¼ã‚¹ãŒå–å¾—ã§ããŸå ´åˆã®ã¿å‡¦ç†ã‚’ç¶šè¡Œ
            if title and url:
                formatted_date = ""
                if date_str:
                    try:
                        date_str_clean = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", date_str).strip()
                        dt_obj = parse_post_date(date_str_clean, jst_now())
                        
                        if dt_obj:
                            formatted_date = format_datetime(dt_obj)
                        else:
                             formatted_date = date_str 
                    except:
                        formatted_date = date_str

                articles_data.append({
                    "URL": url,
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    # æŠ•ç¨¿æ—¥æ™‚ãŒç©ºã®å ´åˆã¯ã€Œå–å¾—ä¸å¯ã€ã‚’ã‚»ãƒƒãƒˆ
                    "æŠ•ç¨¿æ—¥æ™‚": formatted_date if formatted_date else "å–å¾—ä¸å¯", 
                    "ã‚½ãƒ¼ã‚¹": source_text if source_text else "å–å¾—ä¸å¯" # ã‚½ãƒ¼ã‚¹ãŒç©ºã®å ´åˆã¯ã€Œå–å¾—ä¸å¯ã€ã‚’ã‚»ãƒƒãƒˆ
                })
        except Exception as e:
            # å€‹åˆ¥è¨˜äº‹ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
            
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(articles_data)} ä»¶å–å¾—")
    return articles_data

def fetch_article_body_and_comments(base_url: str) -> Tuple[str, int, Optional[str]]:
    # ... (è©³ç´°å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—)
    full_body_parts = []
    comment_count = 0
    extracted_date_str = None
    current_url = base_url
    visited_urls: Set[str] = set()
    MAX_PAGES = 10 

    for page_num in range(1, MAX_PAGES + 1):
        if current_url in visited_urls:
            break
        visited_urls.add(current_url)

        try:
            res = requests.get(current_url, headers=REQ_HEADERS, timeout=20)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            
            article_content = soup.find("article") or soup.find("div", class_="article_body") or soup.find("div", class_=re.compile("article_detail")) 

            if article_content:
                ps = article_content.find_all("p")
                if ps:
                    body_part = "\n".join(p.get_text(strip=True) for p in ps if p.get_text(strip=True))
                    full_body_parts.append(body_part)
                    
                    if page_num == 1:
                        body_text_partial = " ".join(p.get_text(strip=True) for p in ps[:3] if p.get_text(strip=True))
                        match = re.search(r'(\d{1,2}/\d{1,2})\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)(\s*)(\d{1,2}:\d{2})é…ä¿¡', body_text_partial)
                        if match:
                            month_day = match.group(1)
                            time_str = match.group(3)
                            extracted_date_str = f"{month_day} {time_str}"
            
            if page_num == 1:
                comment_button = soup.find("button", attrs={"data-cl-params": re.compile(r"cmtmod")})
                if comment_button:
                    hidden_div = comment_button.find("div", class_="riff-VisuallyHidden__root")
                    text = hidden_div.get_text(strip=True).replace(",", "") if hidden_div else comment_button.get_text(strip=True).replace(",", "")
                    match = re.search(r'(\d+)', text)
                    if match:
                        comment_count = int(match.group(1))

            next_link_tag = soup.find("a", class_=re.compile("sw-Pagination__nextLink"))
            if next_link_tag and next_link_tag.get('href'):
                next_url = next_link_tag['href']
                if next_url.startswith('/'):
                    base_domain = re.match(r'(https?://[^/]+)', base_url)
                    current_url = base_domain.group(0) + next_url if base_domain else next_url
                else:
                    current_url = next_url
                
                print(f"    - æ¬¡ãƒšãƒ¼ã‚¸ã¸ç§»å‹•ä¸­: {page_num + 1} ãƒšãƒ¼ã‚¸ç›®...")
                time.sleep(0.5) 
                
            else:
                break

        except Exception as e:
            print(f"    ! è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ (ãƒšãƒ¼ã‚¸ {page_num}): {e}")
            break
            
    body_text = "\n\n--- ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Š ---\n\n".join(full_body_parts)
    return body_text, comment_count, extracted_date_str


# ====== ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ“ä½œé–¢æ•° (å¤‰æ›´ãªã—) ======

def set_row_height(ws: gspread.Worksheet, row_height_pixels: int):
    try:
        requests = []
        requests.append({
            "updateDimensionProperties": {
                "range": {
                    "sheetId": ws.id,
                    "dimension": "ROWS",
                    "startIndex": 1, 
                    "endIndex": ws.row_count 
                },
                "properties": {
                    "pixelSize": row_height_pixels
                },
                "fields": "pixelSize"
            }
        })
        ws.spreadsheet.batch_update({"requests": requests})
        print(f" 2è¡Œç›®ä»¥é™ã®**è¡Œã®é«˜ã•**ã‚’ {row_height_pixels} ãƒ”ã‚¯ã‚»ãƒ«ã«è¨­å®šã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f" âš ï¸ è¡Œé«˜è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")


def ensure_source_sheet_headers(sh: gspread.Spreadsheet) -> gspread.Worksheet:
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=SOURCE_SHEET_NAME, rows="3000", cols=str(len(YAHOO_SHEET_HEADERS)))
        
    current_headers = ws.row_values(1)
    if current_headers != YAHOO_SHEET_HEADERS:
        ws.update(range_name=f'A1:{gspread.utils.rowcol_to_a1(1, len(YAHOO_SHEET_HEADERS))}', values=[YAHOO_SHEET_HEADERS])
    return ws

def write_news_list_to_source(gc: gspread.Client, articles: list[dict]):
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    worksheet = ensure_source_sheet_headers(sh)
            
    existing_data = worksheet.get_all_values(value_render_option='UNFORMATTED_VALUE')
    existing_urls = set(str(row[0]) for row in existing_data[1:] if len(row) > 0) 
    
    new_data = [[a['URL'], a['ã‚¿ã‚¤ãƒˆãƒ«'], a['æŠ•ç¨¿æ—¥æ™‚'], a['ã‚½ãƒ¼ã‚¹']] for a in articles if a['URL'] not in existing_urls]
    
    if new_data:
        worksheet.append_rows(new_data, value_input_option='USER_ENTERED')
        print(f"  SOURCEã‚·ãƒ¼ãƒˆã« {len(new_data)} ä»¶è¿½è¨˜ã—ã¾ã—ãŸã€‚")
    else:
        print("  SOURCEã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

def sort_yahoo_sheet(gc: gspread.Client):
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        worksheet = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("ã‚½ãƒ¼ãƒˆã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    all_values = worksheet.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        return
        
    header = all_values[0]
    rows = all_values[1:]
    
    now = jst_now()
    def sort_key(row):
        if len(row) > 2:
            dt = parse_post_date(str(row[2]), now)
            return dt if dt else datetime.max.replace(tzinfo=TZ_JST) 
        else:
            return datetime.max.replace(tzinfo=TZ_JST)
        
    sorted_rows = sorted(rows, key=sort_key) 
    
    full_data_to_write = [header] + sorted_rows
    range_end = gspread.utils.rowcol_to_a1(len(full_data_to_write), len(YAHOO_SHEET_HEADERS))
    
    worksheet.update(values=full_data_to_write, range_name=f'A1:{range_end}', value_input_option='USER_ENTERED')
    
    print(" SOURCEã‚·ãƒ¼ãƒˆã‚’æŠ•ç¨¿æ—¥æ™‚ã®å¤ã„é †ã«ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")


# ====== æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ã¨å³æ™‚æ›´æ–° (E, Fåˆ—) (ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—) ======

def fetch_details_and_update_sheet(gc: gspread.Client):
    """ Eåˆ—, Fåˆ—ãŒæœªå…¥åŠ›ã®è¡Œã«å¯¾ã—ã€è©³ç´°å–å¾—ã¨Cåˆ—ã®æ—¥ä»˜è£œå®Œã‚’è¡Œã„ã€è¡Œã”ã¨ã«å³æ™‚æ›´æ–°ã™ã‚‹ """
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("è©³ç´°å–å¾—ã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    all_values = ws.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€è©³ç´°å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
        
    data_rows = all_values[1:]
    update_count = 0
    
    print("\n===== ğŸ“„ ã‚¹ãƒ†ãƒƒãƒ—â‘¡ è¨˜äº‹æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ãƒ»å³æ™‚åæ˜  (E, Fåˆ—) =====")

    for idx, data_row in enumerate(data_rows):
        row_num = idx + 2 
        
        url = str(data_row[0]) if len(data_row) > 0 else ""
        title = str(data_row[1]) if len(data_row) > 1 else "ä¸æ˜"
        post_date_raw = str(data_row[2]) if len(data_row) > 2 else "" # Cåˆ—
        source = str(data_row[3]) if len(data_row) > 3 else ""         # Dåˆ—
        body = str(data_row[4]) if len(data_row) > 4 else ""          # Eåˆ—
        comment_count = str(data_row[5]) if len(data_row) > 5 else ""  # Fåˆ—
        
        needs_details = not body.strip() or not comment_count.strip() or "å–å¾—ä¸å¯" in post_date_raw or not post_date_raw.strip()

        if not needs_details:
            continue
            
        if not url.strip():
            print(f"  - è¡Œ {row_num}: URLãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue

        print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ä¸­...")

        # --- è©³ç´°å–å¾— (C, E, Fåˆ—ã®è£œå®Œ) ---
        fetched_body, fetched_comment_count, extracted_date = fetch_article_body_and_comments(url) 

        new_body = fetched_body if not body.strip() else body
        new_comment_count = fetched_comment_count if not comment_count.strip() or comment_count.strip() == '0' else comment_count

        new_post_date = post_date_raw
        if ("å–å¾—ä¸å¯" in post_date_raw or not post_date_raw.strip()) and extracted_date:
            dt_obj = parse_post_date(extracted_date, jst_now())
            if dt_obj:
                new_post_date = format_datetime(dt_obj)
            else:
                new_post_date = extracted_date # ç”Ÿã®æ–‡å­—åˆ—ã‚’ä¿æŒ

        
        # C, D, E, Fåˆ—ã‚’å³æ™‚æ›´æ–°
        # Dåˆ—(ã‚½ãƒ¼ã‚¹)ã¯ã€A-Dåˆ—å–å¾—æ™‚ã«ã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ã‚‹ã¯ãšãªã®ã§ã€ã“ã“ã§ã¯ãã®ã¾ã¾æ¸¡ã™
        ws.update(
            range_name=f'C{row_num}:F{row_num}', 
            values=[[new_post_date, source, new_body, new_comment_count]],
            value_input_option='USER_ENTERED'
        )
        update_count += 1
        time.sleep(1 + random.random() * 0.5) 

    print(f" âœ… æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆæ•°å–å¾—ã¨æ—¥æ™‚è£œå®Œã‚’ {update_count} è¡Œã«ã¤ã„ã¦å®Ÿè¡Œã—ã€å³æ™‚åæ˜ ã—ã¾ã—ãŸã€‚")


# ====== Geminiåˆ†æã®å®Ÿè¡Œã¨å¼·åˆ¶ä¸­æ–­ (G, H, Iåˆ—) (ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—) ======

def analyze_with_gemini_and_update_sheet(gc: gspread.Client):
    """ Gåˆ—, Håˆ—, Iåˆ—ãŒæœªå…¥åŠ›ã®è¡Œã«å¯¾ã—ã€Geminiåˆ†æã‚’è¡Œã„ã€åˆ†æçµæœã‚’å³æ™‚æ›´æ–°ã™ã‚‹ """
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("Geminiåˆ†æã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    all_values = ws.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€Geminiåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
        
    data_rows = all_values[1:]
    update_count = 0
    
    print("\n===== ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—â‘£ Geminiåˆ†æã®å®Ÿè¡Œãƒ»å³æ™‚åæ˜  (G, H, Iåˆ—) =====")

    for idx, data_row in enumerate(data_rows):
        row_num = idx + 2 
        
        url = str(data_row[0]) if len(data_row) > 0 else ""
        title = str(data_row[1]) if len(data_row) > 1 else "ä¸æ˜"
        body = str(data_row[4]) if len(data_row) > 4 else ""          # Eåˆ—
        company_info = str(data_row[6]) if len(data_row) > 6 else ""   # Gåˆ—
        category = str(data_row[7]) if len(data_row) > 7 else ""       # Håˆ—
        sentiment = str(data_row[8]) if len(data_row) > 8 else ""      # Iåˆ—

        needs_analysis = not company_info.strip() or not category.strip() or not sentiment.strip()

        if not needs_analysis:
            continue
            
        if not body.strip():
            print(f"  - è¡Œ {row_num}: æœ¬æ–‡ãŒãªã„ãŸã‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€N/Aã‚’è¨­å®šã€‚")
            
            ws.update(
                range_name=f'G{row_num}:I{row_num}', 
                values=[['N/A(No Body)', 'N/A', 'N/A']],
                value_input_option='USER_ENTERED'
            )
            update_count += 1
            time.sleep(1)
            continue
            
        if not url.strip():
            print(f"  - è¡Œ {row_num}: URLãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue

        print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): Geminiåˆ†æã‚’å®Ÿè¡Œä¸­...")

        # --- Geminiåˆ†æã‚’å®Ÿè¡Œ (G, H, Iåˆ—) ---
        final_company_info, final_category, final_sentiment, _ = analyze_with_gemini(body) 
        
        ws.update(
            range_name=f'G{row_num}:I{row_num}', 
            values=[[final_company_info, final_category, final_sentiment]],
            value_input_option='USER_ENTERED'
        )
        update_count += 1
        time.sleep(1 + random.random() * 0.5) 

    print(f" âœ… Geminiåˆ†æã‚’ {update_count} è¡Œã«ã¤ã„ã¦å®Ÿè¡Œã—ã€å³æ™‚åæ˜ ã—ã¾ã—ãŸã€‚")


# ====== ãƒ¡ã‚¤ãƒ³å‡¦ç† (å¤‰æ›´ãªã—) ======

def main():
    print("--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ ---")
    
    keywords = load_keywords(KEYWORD_FILE)
    if not keywords:
        sys.exit(0)

    try:
        gc = build_gspread_client()
    except RuntimeError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)
    
    # â‘  ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—: Aï½Dåˆ—ã®å–å¾—ãƒ»è¿½è¨˜ã‚’å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
    for current_keyword in keywords:
        print(f"\n===== ğŸ”‘ ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—: {current_keyword} =====")
        yahoo_news_articles = get_yahoo_news_with_selenium(current_keyword)
        write_news_list_to_source(gc, yahoo_news_articles)
        
    # â‘¡ ã‚¹ãƒ†ãƒƒãƒ—â‘¡ è¨˜äº‹è©³ç´°ï¼ˆæœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆæ•°ï¼‰ã‚’å–å¾—ã—ã€è¡Œã”ã¨ã«å³æ™‚æ›´æ–° (E, Fåˆ—)
    fetch_details_and_update_sheet(gc)
    
    # â‘¢ ã‚¹ãƒ†ãƒƒãƒ—â‘¢ å…¨ã¦ã®è¨˜äº‹ãŒè¿½è¨˜ã•ã‚ŒãŸå¾Œã€ã‚½ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
    print("\n===== ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—â‘¢ å…¨ä»¶ã‚½ãƒ¼ãƒˆå®Ÿè¡Œ =====")
    sort_yahoo_sheet(gc)
    
    # â‘£ ã‚¹ãƒ†ãƒƒãƒ—â‘£ Geminiåˆ†æã‚’å®Ÿè¡Œã—ã€è¡Œã”ã¨ã«å³æ™‚æ›´æ–° (G, H, Iåˆ—)
    analyze_with_gemini_and_update_sheet(gc)
    
    # â‘¤ ã‚¹ãƒ†ãƒƒãƒ—â‘¤ è¡Œã®é«˜ã•ã®èª¿æ•´
    try:
        sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
        ws = sh.worksheet(SOURCE_SHEET_NAME)
        set_row_height(ws, row_height_pixels=21)
    except Exception as e:
        print(f" âš ï¸ æœ€çµ‚çš„ãªè¡Œé«˜è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    # æ­£å¸¸çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    print("\n=== âœ… å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ ===")
    sys.exit(0)

if __name__ == "__main__":
    main()
